{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.094074,"end_time":"2023-09-04T19:23:09.365133","exception":false,"start_time":"2023-09-04T19:23:09.271059","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-12T16:54:40.952226Z","iopub.execute_input":"2023-09-12T16:54:40.952843Z","iopub.status.idle":"2023-09-12T16:54:41.063997Z","shell.execute_reply.started":"2023-09-12T16:54:40.952782Z","shell.execute_reply":"2023-09-12T16:54:41.062739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport spacy\nimport tensorflow as tf\nimport tensorflow_text as text\nfrom transformers import BertTokenizer\n\nnlp = spacy.load(\"en_core_web_lg\")","metadata":{"papermill":{"duration":24.378919,"end_time":"2023-09-04T19:23:33.754836","exception":false,"start_time":"2023-09-04T19:23:09.375917","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-12T16:54:41.065960Z","iopub.execute_input":"2023-09-12T16:54:41.066324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary_df = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_train.csv')\nprompt_df = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_train.csv')\ntest_summary = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv')\ntest_prompts = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv')","metadata":{"papermill":{"duration":0.185061,"end_time":"2023-09-04T19:23:33.950970","exception":false,"start_time":"2023-09-04T19:23:33.765909","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from string import punctuation as punc\nfrom collections import defaultdict\nimport spacy\nfrom string import punctuation as punc\nfrom tqdm import tqdm\nfrom gensim.models import Word2Vec\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\nimport spacy\nfrom textblob import TextBlob\nimport math\nfrom collections import Counter\nfrom nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize\n\ndef extract_lemmas(text):\n    doc = nlp(text)\n    lemmas = []\n\n    for token in doc:\n        if not token.is_stop:\n            lemmas.append(token.lemma_.lower())\n    return ' '.join(lemmas)\n\n\ndef count_punc(text):\n    return len([p for p in text if p in punc])\n\n# Overlap Score\n\ndef tree_depth_from_token(token):\n    \"\"\"Calculate the depth of a subtree rooted at a token.\"\"\"\n    if not list(token.children):\n        return 1\n    else:\n        return 1 + max(tree_depth_from_token(child) for child in token.children)\n\ndef average_dependency_tree_depth(doc):\n    \"\"\"Compute the average depth of the dependency tree.\"\"\"\n    root_tokens = [tok for tok in doc if tok.dep_ == 'ROOT']\n    if not root_tokens:\n        return 0\n    depths = [tree_depth_from_token(token) for token in root_tokens]\n    return sum(depths) / len(depths)\n\ndef process_text(doc):\n    stops = 0\n    num_nouns = 0\n    num_verbs = 0\n    num_adverbs = 0\n    \n    for token in doc:\n        if token.is_stop:\n            stops += 1\n        else:\n            if token.pos_ == \"VERB\":\n                num_verbs += 1\n            elif token.pos_ == \"NOUN\":\n                num_nouns += 1\n            elif token.pos_ == \"ADV\":\n                num_adverbs += 1\n                \n    return stops, num_nouns, num_verbs, num_adverbs\n\ndef syllable_count(word):\n    word = word.lower()\n    count = 0\n    vowels = \"aeiouy\"\n    if word[0] in vowels:\n        count += 1\n    for index in range(1, len(word)):\n        if word[index] in vowels and word[index - 1] not in vowels:\n            count += 1\n    if word.endswith(\"e\"):\n        count -= 1\n    if count == 0:\n        count += 1\n    return count\n\ndef extract_features(row):\n    doc = nlp(row['text'])\n    context = nlp(row['prompt_text'])\n    tokens = word_tokenize(row['text'].lower())\n    context_tokens = word_tokenize(row['prompt_text'].lower())\n    capital_error = sum(1 for sent in doc.sents if not sent.text[0].isupper())\n    \n    # 1. Length of the summary\n    length = len(tokens)\n    \n    # 2. Number of unique words\n    unique_words = len(set(tokens))\n    \n    #3. counter\n    total_tokens = len(tokens) + 1e-11\n    word_counts = Counter(tokens)\n    once = sum(1 for word, count in word_counts.items() if count == 1)\n    twice = sum(1 for word, count in word_counts.items() if count == 2)\n    \n    # 4. Named entities\n    entities = set([ent.text.lower() for ent in doc.ents])\n    context_entities = set([ent.text.lower() for ent in context.ents])\n    num_entities = len(entities)\n    context_num_entities = len(context_entities)\n    \n    # Calculate the overlap between the two sets\n    overlap_entities = context_entities.intersection(entities)\n    \n    # Calculate the coverage score: (number of overlapping entities) / (number of entities in original text)\n    coverage_score = len(overlap_entities) / len(context_entities) if len(context_entities) > 0 else 0\n    num_overlap_entities = len(overlap_entities)\n    \n    named_entity_ratio = float(num_entities/total_tokens)\n    context_entity_ratio = float(context_num_entities/len(context_tokens))\n    \n    # 5. Average word length\n    avg_word_len = round(sum([len(token) for token in tokens])/total_tokens, ndigits=4)\n    context_avg_word_len = round(sum([len(token) for token in context_tokens])/len(context_tokens), ndigits=4)\n    \n    # 6. Summary Polarity\n    text_blob = TextBlob(row['text'])\n    context_blob = TextBlob(row['prompt_text'])\n    sentiment_polarity = text_blob.sentiment.polarity\n    subjectivity = text_blob.subjectivity\n    context_subjectivity = context_blob.subjectivity\n    context_polarity = context_blob.sentiment.polarity\n    error = sum(1 for token in tokens if token not in english)\n    \n    \n    # 7. Stopwords verbs and nouns\n    stops,num_nouns, num_verbs, num_adverbs = process_text(doc)\n    context_stops,context_nouns, context_verbs, context_adverbs = process_text(context)\n    \n    # 8. Numerical entities\n    num_numerical_entities = len([ent for ent in doc.ents if ent.label_ == \"CARDINAL\"])\n    \n    \n    # 9. Sentece count\n    sentences = [sent.text for sent in doc.sents]\n    sentence = len(sentences) + 1e-12\n    avg_sentence = float(sentence / total_tokens)\n    avg_unique_sentence = float(sentence /unique_words) if unique_words > 0 else 0.0\n    \n    avg_sentence_length = sum(len(sent) for sent in sentences) / sentence\n    max_sentence_length = max(len(sent) for sent in sentences)\n    min_sentence_length = min(len(sent) for sent in sentences)\n    std_sentence_length = (sum((len(sent) - avg_sentence_length)**2 for sent in sentences) / sentence)**0.5\n    \n    # 10 Tree Depth\n    average_tree_depth = average_dependency_tree_depth(doc)\n    prob_word = [count/total_tokens for word, count in word_counts.items()]\n    results = {\n        \"nsubj\": 0,\n        \"amod\": 0,\n        \"advmod\": 0,\n        \"xcomp\": 0,\n        \"acomp\": 0,\n        \"past\": 0,\n        \"present\": 0\n    }\n    \n    num_syllables = 0\n    \n    for token in doc:\n        if \"VERB\" in token.pos_:\n            if \"VBD\" in token.tag_ or \"VBN\" in token.tag_:\n                results[\"past\"] += 1\n            elif \"VBG\" in token.tag_ or \"VBP\" in token.tag_ or \"VBZ\" in token.tag_:\n                results[\"present\"] += 1\n        # Check if the token's dependency is one of the desired dependencies\n        if token.dep_ in results:\n            results[token.dep_] += 1\n            \n        if token.is_alpha:\n            num_syllables += syllable_count(token.text.lower())\n    \n    \n    # Unigram similarity\n    summary_set = set(tokens) if length > 0 else set()\n    context_set = set(context_tokens) if len(context_tokens) > 0 else set()\n    intersection = len(context_set.intersection(summary_set))\n    difference = len(context_set.difference(summary_set))\n    union = len(context_set.union(summary_set))\n    precision = float(intersection /unique_words) if unique_words > 0 else 0.0\n    recall = float(intersection / len(context_set)) if len(context_set) > 0 else 0.0\n    jaccard_similarity = float(intersection/union) if union > 0 else 0.0\n    overlap_score = intersection/float(min(len(context_set), len(summary_set)) + 1e-11)\n    f1_score = float(2 * ((precision * recall)/(precision + recall + 1e-11)))\n    \n    \n    # Bigram Similarity\n    answer_bigrams = set(list(ngrams(tokens, 2))) if length > 1 else set()\n    context_bigrams = set(list(ngrams(context_tokens, 2))) if len(context_tokens) > 1 else set()\n    intersection_bigram = len(answer_bigrams.intersection(context_bigrams))\n    union_bigram = len(answer_bigrams.union(context_bigrams))\n    precision_bigram = intersection_bigram /len(answer_bigrams) if len(answer_bigrams) > 0 else 0.0\n    recall_bigram = intersection_bigram / len(context_bigrams) if len(context_bigrams) > 0 else 0.0\n    jaccard_bigram = intersection_bigram/union_bigram if union_bigram > 0 else 0.0\n    bigram_overlap = intersection_bigram/float(min(len(context_bigrams), len(answer_bigrams)) + 1e-11)    \n    f1_bigram = 2 * ((precision_bigram * recall_bigram)/(precision_bigram + recall_bigram + 1e-11))\n    \n    # Trigram Similarity\n    answer_trigrams = set(list(ngrams(tokens, 3))) if length > 1 else set()\n    context_trigrams = set(list(ngrams(context_tokens, 3))) if len(context_tokens) > 1 else set()\n    intersection_trigram = len(answer_trigrams.intersection(context_trigrams))\n    union_trigram = len(answer_trigrams.union(context_trigrams))\n    precision_trigram = intersection_trigram / len(answer_trigrams) if len(answer_trigrams) > 0 else 0.0\n    recall_trigram = intersection_trigram / len(context_trigrams) if len(context_trigrams) > 0 else 0.0\n    jaccard_trigram = intersection_trigram/union_trigram if union_trigram > 0 else 0.0\n    trigram_overlap = intersection_trigram/float(min(len(context_trigrams), len(answer_trigrams)) + 1e-11)    \n    f1_trigram = 2 * ((precision_trigram * recall_trigram)/(precision_trigram + recall_trigram + 1e-11))\n    \n    # Readability\n    ASL = float(total_tokens / sentence)\n    ASW = float(num_syllables / total_tokens)\n\n    flesch_reading_ease = 206.835 - (1.015 * ASL) - (84.6 * ASW)\n    flesch_kincaid = (0.39 * ASL) + (11.8 * ASW) - 15.59\n    \n    # Automated Reading Index\n    characters = len(\"\".join(tokens))\n    summary_ari = 4.71 * (characters / total_tokens) + 0.5 * (total_tokens / sentence) - 21.43\n    \n    # Coleman-Liau Index Readability\n    # Where: L is the average number of letters per 100 words, S is the average number of sentences per 100 words\n    L = (characters / total_tokens) * 100\n    S = (sentence / total_tokens) * 100\n    coleman_index = (0.0588 * L) - (0.296 * S) - 15.8\n    \n    # Organizing features in a dictionary\n    features = {\n        'length': length,\n        'num_chars': len(row['text']),\n        \"capital_error\": capital_error,\n        \"intersection_bigram\": intersection_bigram,\n        \"union_bigram\": union_bigram,\n        \"jaccard_bigram\": jaccard_bigram,\n        'recall_bigram': recall_bigram,\n        \"precision_bigram\": precision_bigram,\n        \"f1_bigram\": f1_bigram,\n        'bigram_overlap': bigram_overlap,\n        'sentence': sentence,\n        'avg_sentence_length': avg_sentence_length, \n        'max_sentence_length': max_sentence_length,\n        'min_sentence_length':min_sentence_length,\n        \"std_sentence_length\": std_sentence_length,\n        \"avg_sentence\": avg_sentence,\n        'avg_unique_sentence':avg_unique_sentence,\n        \"context_avg_word_len\": context_avg_word_len,\n        \"error\": error,\n        'unique_words': unique_words,\n        'num_entities': num_entities,\n        \"context_num_entities\": context_num_entities,\n        'coverage_score': coverage_score,\n        'num_overlap_entities': num_overlap_entities,\n        'avg_word_len': avg_word_len,\n        'intersection_trigram': intersection_trigram,\n        'union_trigram': union_trigram,\n        'recall_trigram': recall_trigram,\n        'precision_trigram': precision_trigram,\n        'jaccard_trigram': jaccard_trigram,\n        'trigram_overlap': trigram_overlap,\n        'f1_trigram': f1_trigram,\n        \"stops\": stops,\n        \"num_nouns\": num_nouns,\n        \"num_verbs\": num_verbs,\n        \"num_adverbs\": num_adverbs,\n        \"context_nouns\": context_nouns,\n        \"context_stops\": context_stops, \n        'context_verbs': context_verbs,\n        'context_adverbs': context_adverbs,\n        \"num_numerical_entities\": num_numerical_entities,\n        \"sentiment_polarity\": sentiment_polarity,\n        'context_polarity': context_polarity, \n        \"nsubj\": results['nsubj'],\n        \"amod\": results['amod'],\n        \"advmod\": results['advmod'],\n        \"xcomp\": results['xcomp'],\n        \"acomp\": results['acomp'],\n        \"past\": results[\"past\"],\n        \"present\": results[\"present\"],\n        \"average_dependency_tree_depth\": average_tree_depth,\n        \"TTR\": unique_words / (total_tokens + 1e-8),\n        \"RTTR\": unique_words / (math.sqrt(total_tokens) + 1e-8),\n        \"CTTR\": unique_words / (math.sqrt(total_tokens / 2) + 1e-8),\n        # MATTR would require a window-based approach and is thus more involved\n        \"Herdan's C\": math.log(unique_words) / (math.log(total_tokens) + 1e-8),\n        \"Dugast's U\": math.log(total_tokens)**2 / ((math.log(total_tokens) - math.log(unique_words)) + 1e-8),\n        \"Honoré's H\": 100 * math.log(total_tokens) / (1 - once/(unique_words + 1e-8)),\n        \"Entropy\": -sum(p * math.log(p) for p in prob_word),\n        \"Sichel’s S\": twice,\n        'named_entity_ratio': named_entity_ratio,\n        'context_entity_ratio': context_entity_ratio,\n        'flesch_reading_ease': flesch_reading_ease,\n        'flesch_kincaid': flesch_kincaid,\n        'summary_ari':summary_ari,\n        'coleman_index': coleman_index,\n        \"Simpson’s D\": sum((count/total_tokens)**2 for count in word_counts.values()),\n        'intersection': intersection,\n        \"subjectivity\": subjectivity,\n        \"context_subjectivity\": context_subjectivity,\n        'difference': difference,\n        'union': union,\n        'overlap_score': overlap_score,\n        'recall': recall,\n        'precision': precision,\n        'f1_score': f1_score,\n        'jaccard_similarity': jaccard_similarity,\n    }\n    \n    return features\n\ndef compute_similarity(row):\n    source_vector = nlp(row['prompt_text']).vector\n    answer_vector = nlp(row['text']).vector\n    embedding_similarity = cosine_similarity([source_vector], [answer_vector])[0][0]\n    euclidean = euclidean_distances([source_vector], [answer_vector])[0][0]\n    pearson = np.corrcoef(source_vector.ravel(), answer_vector.ravel())[0, 1]\n    \n    return pd.Series([embedding_similarity, euclidean, pearson])\n\ndef compute_statistical_measures(embedding):\n    return np.mean(embedding), np.std(embedding), np.min(embedding), np.max(embedding)\n\ndef get_embeddings(row, vectorizer, transformer):\n    bag_of_words = transformer.fit_transform(vectorizer.transform([row['lemmas']])).toarray()\n    context_words = transformer.fit_transform(vectorizer.transform([row['full_text']])).toarray()\n    \n    difference = context_words - bag_of_words\n    addition = bag_of_words + context_words\n    bags_ratio = context_words / (bag_of_words + 1e-13)\n        \n    word_features = compute_statistical_measures(bag_of_words)\n    difference_features = compute_statistical_measures(difference)\n    addition_features = compute_statistical_measures(addition)\n    ratio_features = compute_statistical_measures(bags_ratio)\n    \n    similarity = cosine_similarity(context_words, bag_of_words)[0][0]\n    distance = euclidean_distances(context_words, bag_of_words)[0][0]\n    manhattan_distances = np.sum(np.abs(context_words.ravel() - bag_of_words.ravel()))\n    \n    return np.hstack((word_features,difference_features,\n                      addition_features,ratio_features, \n                      similarity, distance, manhattan_distances\n                     ))\n    \ndef vectorize_tokens(train_df, ngram_range=(1,1)):\n    vectorizer = CountVectorizer(lowercase=True, ngram_range=ngram_range)\n    transformer = TfidfTransformer()\n    vectorizer.fit(train_df['lemmas'])\n    embed = np.asarray(train_df.apply(get_embeddings, axis=1, args=(vectorizer, transformer)).tolist(), dtype=np.float32) \n    \n    return embed","metadata":{"papermill":{"duration":0.9004,"end_time":"2023-09-04T19:23:34.861798","exception":false,"start_time":"2023-09-04T19:23:33.961398","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.decomposition import TruncatedSVD,PCA\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n\ndef preprocess_data(summary_df, prompt_df):\n    train_df = summary_df.merge(right=prompt_df, how='inner', on='prompt_id')\n\n    print('processing text embeddings......')\n    train_df = train_df.assign(\n      full_text='Title:\\n' + train_df['prompt_title'] +\n                '\\nQuestion:\\n' + train_df['prompt_question'] +\n                '\\nText:\\n' + train_df['prompt_text']\n      )\n    \n    \n    train_df['lemmas'] = train_df['text'].apply(extract_lemmas)\n    train_df['full_text'] = train_df['full_text'].apply(extract_lemmas)\n    \n    print('Fitting vectorizer')\n    embed = vectorize_tokens(train_df, ngram_range=(1,1))\n    embed1 = vectorize_tokens(train_df, ngram_range=(2,2))\n    \n    # processing lemmas\n    print('extracting features.....')\n    data = pd.DataFrame(train_df.apply(extract_features, axis=1).tolist())\n    \n    print('processing features.....')\n    # Process other columns\n    train_df['summary_punctuation'] = train_df['text'].apply(count_punc)\n    \n    cols = ['summary_punctuation','content']\n    cols1 = ['summary_punctuation', 'wording']\n\n    similarity_df = train_df.apply(compute_similarity, axis=1)\n    similarity = similarity_df.values\n    \n    content_bag = np.hstack((embed, embed1, data.values, similarity, train_df[cols].values))\n    wording_bag = np.hstack((data.values, similarity, train_df[cols1].values))\n    \n    print('collecting outputs.........')\n    student_ids = train_df['student_id'].values\n\n    return content_bag, wording_bag, student_ids","metadata":{"papermill":{"duration":0.032381,"end_time":"2023-09-04T19:23:34.904770","exception":false,"start_time":"2023-09-04T19:23:34.872389","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import words\nenglish = set(words.words())","metadata":{"papermill":{"duration":0.176365,"end_time":"2023-09-04T19:23:35.091256","exception":false,"start_time":"2023-09-04T19:23:34.914891","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport time\n\nstart_time = time.time()\n\ncontent_bag, wording_bag, student_ids = preprocess_data(summary_df,prompt_df)\n\nend_time = time.time()\nelapsed_time = end_time - start_time\n\nprint(f\"Preprocessed function executed in: {elapsed_time:.2f} seconds\")","metadata":{"papermill":{"duration":3823.336513,"end_time":"2023-09-04T20:27:18.437851","exception":false,"start_time":"2023-09-04T19:23:35.101338","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_data(df):\n    X_train,X_test,y_train,y_test = train_test_split(df[:,:-1] ,df[:,-1],test_size=0.1,shuffle=True,random_state=11)\n    return X_train,X_test,y_train,y_test","metadata":{"papermill":{"duration":0.034321,"end_time":"2023-09-04T20:27:18.483482","exception":false,"start_time":"2023-09-04T20:27:18.449161","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mcrmse(y_true, y_pred):\n    return np.mean(np.sqrt(np.mean((y_true - y_pred)**2, axis=0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport lightgbm as lgb\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\n\ndef train_model(df):\n    X_train,X_test,y_train,y_test = split_data(df)\n    start_time = time.time()\n\n    best_params = {\n        'learning_rate': 0.01,\n        'n_estimators': 600,\n        'max_depth': 11,\n        'subsample': 0.6,\n        'colsample_bytree': 0.6,\n        'num_leaves': 31,\n        'objective': 'regression',\n        'metric': 'rmse'\n    }\n\n    best_models = []\n\n    score = []\n    kfold = KFold(n_splits=20, shuffle=True, random_state=64)\n    for train_idx, test_idx in tqdm(kfold.split(X_train, y_train), desc='training model'):\n        X, y = X_train[train_idx], y_train[train_idx]\n        test,test_y = X_train[test_idx], y_train[test_idx]\n        pipeline = make_pipeline(\n            MinMaxScaler(feature_range=(-10,10)),\n            lgb.LGBMRegressor(**best_params, random_state=64)\n        )\n        \n        pipeline.fit(X, y)\n        score.append(np.sqrt(mean_squared_error(test_y,pipeline.predict(test))))\n        best_models.append(pipeline)\n\n    print(f'Train Mean Score: {np.mean(score):.4f}')\n    \n    best_scores = []\n    for i in range(len(best_models)):\n        best_model = best_models[i]\n        y_preds = best_model.predict(X_test)\n        best_scores.append(np.sqrt(mean_squared_error(y_test,y_preds)))\n    print(f'Evaluation Mean Score: {np.mean(best_scores):.4f}')\n    \n    best_model_idx = np.argmin(best_scores)\n    best_model = best_models[best_model_idx]\n    \n    y_preds = best_model.predict(X_test)\n    score = np.sqrt(mean_squared_error(y_test,y_preds))\n    \n    end_time = time.time()\n    elapsed_time = end_time - start_time \n    print(f\"Preprocessed function executed in: {elapsed_time:.2f} seconds\")\n    print(f'Best Model Score: {score:.4f}')\n    \n    return best_model, y_test, y_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"content_model, content_test, content_preds = train_model(content_bag)","metadata":{"papermill":{"duration":1275.871685,"end_time":"2023-09-04T20:48:34.470144","exception":false,"start_time":"2023-09-04T20:27:18.598459","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wording_model, wording_test, wording_preds  = train_model(wording_bag)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = np.hstack((content_test.reshape((-1,1)), wording_test.reshape((-1,1))))\npreds = np.hstack((content_preds.reshape((-1,1)), wording_preds.reshape((-1,1))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mcrmse(y_test, preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 0.4700925199890056","metadata":{}},{"cell_type":"markdown","source":"### Preprocessing for text data","metadata":{"papermill":{"duration":0.013261,"end_time":"2023-09-04T20:48:34.956918","exception":false,"start_time":"2023-09-04T20:48:34.943657","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.decomposition import TruncatedSVD,PCA\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n\ndef preprocess_data(summary_df, prompt_df):\n    train_df = summary_df.merge(right=prompt_df, how='inner', on='prompt_id')\n\n    print('processing text embeddings......')\n    train_df = train_df.assign(\n      full_text='Title:\\n' + train_df['prompt_title'] +\n                '\\nQuestion:\\n' + train_df['prompt_question'] +\n                '\\nText:\\n' + train_df['prompt_text']\n      )\n    \n    train_df['lemmas'] = train_df['text'].apply(extract_lemmas)\n    train_df['full_text'] = train_df['full_text'].apply(extract_lemmas)\n    \n    print('Fitting vectorizer')\n    embed = vectorize_tokens(train_df, ngram_range=(1,1))\n    embed1 = vectorize_tokens(train_df, ngram_range=(2,2))\n    \n    # processing lemmas\n    print('extracting features.....')\n    data = pd.DataFrame(train_df.apply(extract_features, axis=1).tolist())\n    \n    print('processing features.....')\n    # Process other columns\n    train_df['summary_punctuation'] = train_df['text'].apply(count_punc)\n    \n    cols = ['summary_punctuation']\n\n    similarity_df = train_df.apply(compute_similarity, axis=1)\n    similarity = similarity_df.values\n    \n    content_bag = np.hstack((embed, embed1, data.values, similarity, train_df[cols].values))\n    wording_bag = np.hstack((data.values, similarity, train_df[cols].values))\n    \n    print('collecting outputs.........')\n    student_ids = train_df['student_id'].values\n\n    return content_bag, wording_bag, student_ids","metadata":{"papermill":{"duration":0.036424,"end_time":"2023-09-04T20:48:35.007448","exception":false,"start_time":"2023-09-04T20:48:34.971024","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"content, wording,ids = preprocess_data(test_summary, test_prompts)","metadata":{"papermill":{"duration":0.302293,"end_time":"2023-09-04T20:48:35.326067","exception":false,"start_time":"2023-09-04T20:48:35.023774","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cont_preds = content_model.predict(content).reshape((-1,1))\nword_preds = wording_model.predict(wording).reshape((-1,1))\ny_preds = np.hstack((cont_preds, word_preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.DataFrame(y_preds,columns=['content','wording'],index=ids).reset_index()\ndf_test = df_test.rename({'index':'student_id'},axis=1)","metadata":{"papermill":{"duration":0.027529,"end_time":"2023-09-04T20:48:35.610715","exception":false,"start_time":"2023-09-04T20:48:35.583186","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def is_valid_float(x):\n    return isinstance(x, float) and x == x  # This checks that x is not NaN since NaN != NaN in Python.\n\ncols_to_check = ['wording', 'content']\ndf_test[cols_to_check] = df_test[cols_to_check].applymap(lambda x: x if is_valid_float(x) else 0.0)\n\ndf_test.to_csv(\"submission.csv\", index=False)","metadata":{"papermill":{"duration":0.032411,"end_time":"2023-09-04T20:48:35.657275","exception":false,"start_time":"2023-09-04T20:48:35.624864","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}