{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "127227a8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-09-09T04:24:09.332300Z",
     "iopub.status.busy": "2023-09-09T04:24:09.331890Z",
     "iopub.status.idle": "2023-09-09T04:24:09.400946Z",
     "shell.execute_reply": "2023-09-09T04:24:09.399821Z"
    },
    "papermill": {
     "duration": 0.0859,
     "end_time": "2023-09-09T04:24:09.404403",
     "exception": false,
     "start_time": "2023-09-09T04:24:09.318503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/commonlit-evaluate-student-summaries/sample_submission.csv\n",
      "/kaggle/input/commonlit-evaluate-student-summaries/prompts_train.csv\n",
      "/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv\n",
      "/kaggle/input/commonlit-evaluate-student-summaries/summaries_train.csv\n",
      "/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/meta.json\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/__init__.py\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/tokenizer\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/LICENSES_SOURCES\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/LICENSE\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/README.md\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/meta.json\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/accuracy.json\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/config.cfg\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/attribute_ruler/patterns\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/vocab/vectors\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/vocab/key2row\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/vocab/vectors.cfg\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/vocab/strings.json\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/vocab/lookups.bin\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/parser/model\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/parser/moves\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/parser/cfg\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/senter/model\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/senter/cfg\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/ner/model\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/ner/moves\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/ner/cfg\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/lemmatizer/lookups/lookups.bin\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/tagger/model\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/tagger/cfg\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/tok2vec/model\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/tok2vec/cfg\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e6f4b25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T04:24:09.425507Z",
     "iopub.status.busy": "2023-09-09T04:24:09.425025Z",
     "iopub.status.idle": "2023-09-09T04:24:30.894134Z",
     "shell.execute_reply": "2023-09-09T04:24:30.892886Z"
    },
    "papermill": {
     "duration": 21.483064,
     "end_time": "2023-09-09T04:24:30.896984",
     "exception": false,
     "start_time": "2023-09-09T04:24:09.413920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40e7cead",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T04:24:30.918317Z",
     "iopub.status.busy": "2023-09-09T04:24:30.917564Z",
     "iopub.status.idle": "2023-09-09T04:24:31.037826Z",
     "shell.execute_reply": "2023-09-09T04:24:31.036607Z"
    },
    "papermill": {
     "duration": 0.133878,
     "end_time": "2023-09-09T04:24:31.040555",
     "exception": false,
     "start_time": "2023-09-09T04:24:30.906677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_df = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_train.csv')\n",
    "prompt_df = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_train.csv')\n",
    "test_summary = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv')\n",
    "test_prompts = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8d37890",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T04:24:31.062482Z",
     "iopub.status.busy": "2023-09-09T04:24:31.062039Z",
     "iopub.status.idle": "2023-09-09T04:24:31.816490Z",
     "shell.execute_reply": "2023-09-09T04:24:31.815570Z"
    },
    "papermill": {
     "duration": 0.768698,
     "end_time": "2023-09-09T04:24:31.819150",
     "exception": false,
     "start_time": "2023-09-09T04:24:31.050452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from string import punctuation as punc\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from string import punctuation as punc\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "import math\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def extract_lemmas(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = []\n",
    "\n",
    "    for token in doc:\n",
    "        if not token.is_stop:\n",
    "            lemmas.append(token.text.lower())\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "\n",
    "def count_punc(text):\n",
    "    return len([p for p in text if p in punc])\n",
    "\n",
    "# Overlap Score\n",
    "\n",
    "def tree_depth_from_token(token):\n",
    "    \"\"\"Calculate the depth of a subtree rooted at a token.\"\"\"\n",
    "    if not list(token.children):\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 + max(tree_depth_from_token(child) for child in token.children)\n",
    "\n",
    "def average_dependency_tree_depth(doc):\n",
    "    \"\"\"Compute the average depth of the dependency tree.\"\"\"\n",
    "    root_tokens = [tok for tok in doc if tok.dep_ == 'ROOT']\n",
    "    if not root_tokens:\n",
    "        return 0\n",
    "    depths = [tree_depth_from_token(token) for token in root_tokens]\n",
    "    return sum(depths) / len(depths)\n",
    "\n",
    "def process_text(doc):\n",
    "    stops = 0\n",
    "    num_nouns = 0\n",
    "    num_verbs = 0\n",
    "    num_adverbs = 0\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.is_stop:\n",
    "            stops += 1\n",
    "        else:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                num_verbs += 1\n",
    "            elif token.pos_ == \"NOUN\":\n",
    "                num_nouns += 1\n",
    "            elif token.pos_ == \"ADV\":\n",
    "                num_adverbs += 1\n",
    "                \n",
    "    return stops, num_nouns, num_verbs, num_adverbs\n",
    "\n",
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def extract_features(row):\n",
    "    doc = nlp(row['text'])\n",
    "    context = nlp(row['prompt_text'])\n",
    "    tokens = word_tokenize(row['text'].lower())\n",
    "    context_tokens = word_tokenize(row['prompt_text'].lower())\n",
    "    capital_error = sum(1 for sent in doc.sents if not sent.text[0].isupper())\n",
    "    \n",
    "    # 1. Length of the summary\n",
    "    length = len(tokens)\n",
    "    \n",
    "    # 2. Number of unique words\n",
    "    unique_words = len(set(tokens))\n",
    "    \n",
    "    #3. counter\n",
    "    total_tokens = len(tokens) + 1e-11\n",
    "    word_counts = Counter(tokens)\n",
    "    once = sum(1 for word, count in word_counts.items() if count == 1)\n",
    "    twice = sum(1 for word, count in word_counts.items() if count == 2)\n",
    "    \n",
    "    # 4. Named entities\n",
    "    entities = set([ent.text.lower() for ent in doc.ents])\n",
    "    context_entities = set([ent.text.lower() for ent in context.ents])\n",
    "    num_entities = len(entities)\n",
    "    context_num_entities = len(context_entities)\n",
    "    \n",
    "    # Calculate the overlap between the two sets\n",
    "    overlap_entities = context_entities.intersection(entities)\n",
    "    \n",
    "    # Calculate the coverage score: (number of overlapping entities) / (number of entities in original text)\n",
    "    coverage_score = len(overlap_entities) / len(context_entities) if len(context_entities) > 0 else 0\n",
    "    num_overlap_entities = len(overlap_entities)\n",
    "    \n",
    "    named_entity_ratio = float(num_entities/total_tokens)\n",
    "    context_entity_ratio = float(context_num_entities/len(context_tokens))\n",
    "    \n",
    "    # 5. Average word length\n",
    "    avg_word_len = round(sum([len(token) for token in tokens])/total_tokens, ndigits=4)\n",
    "    \n",
    "    # 6. Summary Polarity\n",
    "    text_blob = TextBlob(row['text'])\n",
    "    sentiment_polarity = text_blob.sentiment.polarity\n",
    "    subjectivity = text_blob.subjectivity\n",
    "    error = sum(1 for token in tokens if token not in english)\n",
    "    \n",
    "    \n",
    "    # 7. Stopwords verbs and nouns\n",
    "    stops,num_nouns, num_verbs, num_adverbs = process_text(doc)\n",
    "    context_stops,_, context_verbs, _ = process_text(context)\n",
    "    \n",
    "    # 8. Numerical entities\n",
    "    num_numerical_entities = len([ent for ent in doc.ents if ent.label_ == \"CARDINAL\"])\n",
    "    \n",
    "    # 9. Sentece count\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    sentence = len(sentences) + 1e-12\n",
    "    avg_sentence = float(sentence / total_tokens)\n",
    "    avg_unique_sentence = float(sentence /unique_words) if unique_words > 0 else 0.0\n",
    "    \n",
    "    avg_sentence_length = sum(len(sent) for sent in sentences) / sentence\n",
    "    max_sentence_length = max(len(sent) for sent in sentences)\n",
    "    min_sentence_length = min(len(sent) for sent in sentences)\n",
    "    std_sentence_length = (sum((len(sent) - avg_sentence_length)**2 for sent in sentences) / sentence)**0.5\n",
    "    \n",
    "    # 10 Tree Depth\n",
    "    average_tree_depth = average_dependency_tree_depth(doc)\n",
    "    prob_word = [count/total_tokens for word, count in word_counts.items()]\n",
    "    results = {\n",
    "        \"nsubj\": 0,\n",
    "        \"amod\": 0,\n",
    "        \"advmod\": 0,\n",
    "        \"xcomp\": 0,\n",
    "        \"acomp\": 0,\n",
    "        \"past\": 0,\n",
    "        \"present\": 0\n",
    "    }\n",
    "    \n",
    "    num_syllables = 0\n",
    "    \n",
    "    for token in doc:\n",
    "        if \"VERB\" in token.pos_:\n",
    "            if \"VBD\" in token.tag_ or \"VBN\" in token.tag_:\n",
    "                results[\"past\"] += 1\n",
    "            elif \"VBG\" in token.tag_ or \"VBP\" in token.tag_ or \"VBZ\" in token.tag_:\n",
    "                results[\"present\"] += 1\n",
    "        # Check if the token's dependency is one of the desired dependencies\n",
    "        if token.dep_ in results:\n",
    "            results[token.dep_] += 1\n",
    "            \n",
    "        if token.is_alpha:\n",
    "            num_syllables += syllable_count(token.text.lower())\n",
    "    \n",
    "    \n",
    "    # Unigram similarity\n",
    "    summary_set = set(tokens) if length > 0 else set()\n",
    "    context_set = set(context_tokens) if len(context_tokens) > 0 else set()\n",
    "    intersection = len(context_set.intersection(summary_set))\n",
    "    difference = len(context_set.difference(summary_set))\n",
    "    union = len(context_set.union(summary_set))\n",
    "    precision = float(intersection /unique_words) if unique_words > 0 else 0.0\n",
    "    recall = float(intersection / len(context_set)) if len(context_set) > 0 else 0.0\n",
    "    jaccard_similarity = float(intersection/union) if union > 0 else 0.0\n",
    "    overlap_score = intersection/float(min(len(context_set), len(summary_set)) + 1e-11)\n",
    "    f1_score = float(2 * ((precision * recall)/(precision + recall + 1e-11)))\n",
    "    \n",
    "    \n",
    "    # Bigram Similarity\n",
    "    answer_bigrams = set(list(ngrams(tokens, 2))) if length > 1 else set()\n",
    "    context_bigrams = set(list(ngrams(context_tokens, 2))) if len(context_tokens) > 1 else set()\n",
    "    intersection_bigram = len(answer_bigrams.intersection(context_bigrams))\n",
    "    union_bigram = len(answer_bigrams.union(context_bigrams))\n",
    "    precision_bigram = intersection_bigram /len(answer_bigrams) if len(answer_bigrams) > 0 else 0.0\n",
    "    recall_bigram = intersection_bigram / len(context_bigrams) if len(context_bigrams) > 0 else 0.0\n",
    "    jaccard_bigram = intersection_bigram/union_bigram if union_bigram > 0 else 0.0\n",
    "    bigram_overlap = intersection_bigram/float(min(len(context_bigrams), len(answer_bigrams)) + 1e-11)    \n",
    "    f1_bigram = 2 * ((precision_bigram * recall_bigram)/(precision_bigram + recall_bigram + 1e-11))\n",
    "    \n",
    "    # Trigram Similarity\n",
    "    answer_trigrams = set(list(ngrams(tokens, 3))) if length > 1 else set()\n",
    "    context_trigrams = set(list(ngrams(context_tokens, 3))) if len(context_tokens) > 1 else set()\n",
    "    intersection_trigram = len(answer_trigrams.intersection(context_trigrams))\n",
    "    union_trigram = len(answer_trigrams.union(context_trigrams))\n",
    "    precision_trigram = intersection_trigram / len(answer_trigrams) if len(answer_trigrams) > 0 else 0.0\n",
    "    recall_trigram = intersection_trigram / len(context_trigrams) if len(context_trigrams) > 0 else 0.0\n",
    "    jaccard_trigram = intersection_trigram/union_trigram if union_trigram > 0 else 0.0\n",
    "    trigram_overlap = intersection_trigram/float(min(len(context_trigrams), len(answer_trigrams)) + 1e-11)    \n",
    "    f1_trigram = 2 * ((precision_trigram * recall_trigram)/(precision_trigram + recall_trigram + 1e-11))\n",
    "    \n",
    "    # Readability\n",
    "    ASL = float(total_tokens / sentence)\n",
    "    ASW = float(num_syllables / total_tokens)\n",
    "\n",
    "    flesch_reading_ease = 206.835 - (1.015 * ASL) - (84.6 * ASW)\n",
    "    flesch_kincaid = (0.39 * ASL) + (11.8 * ASW) - 15.59\n",
    "    \n",
    "    # Automated Reading Index\n",
    "    characters = len(\"\".join(tokens))\n",
    "    summary_ari = 4.71 * (characters / total_tokens) + 0.5 * (total_tokens / sentence) - 21.43\n",
    "    \n",
    "    # Coleman-Liau Index Readability\n",
    "    # Where: L is the average number of letters per 100 words, S is the average number of sentences per 100 words\n",
    "    L = (characters / total_tokens) * 100\n",
    "    S = (sentence / total_tokens) * 100\n",
    "    coleman_index = (0.0588 * L) - (0.296 * S) - 15.8\n",
    "    \n",
    "    # Organizing features in a dictionary\n",
    "    features = {\n",
    "        'length': length,\n",
    "        'num_chars': len(row['text']),\n",
    "        \"capital_error\": capital_error,\n",
    "        \"intersection_bigram\": intersection_bigram,\n",
    "        \"union_bigram\": union_bigram,\n",
    "        \"jaccard_bigram\": jaccard_bigram,\n",
    "        'recall_bigram': recall_bigram,\n",
    "        \"precision_bigram\": precision_bigram,\n",
    "        \"f1_bigram\": f1_bigram,\n",
    "        'bigram_overlap': bigram_overlap,\n",
    "        'sentence': sentence,\n",
    "        'avg_sentence_length': avg_sentence_length, \n",
    "        'max_sentence_length': max_sentence_length,\n",
    "        'min_sentence_length':min_sentence_length,\n",
    "        \"std_sentence_length\": std_sentence_length,\n",
    "        \"avg_sentence\": avg_sentence,\n",
    "        'avg_unique_sentence':avg_unique_sentence,\n",
    "        \"error\": error,\n",
    "        'unique_words': unique_words,\n",
    "        'num_entities': num_entities,\n",
    "        \"context_num_entities\": context_num_entities,\n",
    "        'coverage_score': coverage_score,\n",
    "        'num_overlap_entities': num_overlap_entities,\n",
    "        'avg_word_len': avg_word_len,\n",
    "        'intersection_trigram': intersection_trigram,\n",
    "        'union_trigram': union_trigram,\n",
    "        'recall_trigram': recall_trigram,\n",
    "        'precision_trigram': precision_trigram,\n",
    "        'jaccard_trigram': jaccard_trigram,\n",
    "        'trigram_overlap': trigram_overlap,\n",
    "        'f1_trigram': f1_trigram,\n",
    "        \"stops\": stops,\n",
    "        \"num_nouns\": num_nouns,\n",
    "        \"num_verbs\": num_verbs,\n",
    "        \"num_adverbs\": num_adverbs,\n",
    "        \"num_numerical_entities\": num_numerical_entities,\n",
    "        \"sentiment_polarity\": sentiment_polarity,\n",
    "        \"nsubj\": results['nsubj'],\n",
    "        \"amod\": results['amod'],\n",
    "        \"advmod\": results['advmod'],\n",
    "        \"xcomp\": results['xcomp'],\n",
    "        \"acomp\": results['acomp'],\n",
    "        \"past\": results[\"past\"],\n",
    "        \"present\": results[\"present\"],\n",
    "        \"average_dependency_tree_depth\": average_tree_depth,\n",
    "        \"TTR\": unique_words / (total_tokens + 1e-8),\n",
    "        \"RTTR\": unique_words / (math.sqrt(total_tokens) + 1e-8),\n",
    "        \"CTTR\": unique_words / (math.sqrt(total_tokens / 2) + 1e-8),\n",
    "        # MATTR would require a window-based approach and is thus more involved\n",
    "        \"Herdan's C\": math.log(unique_words) / (math.log(total_tokens) + 1e-8),\n",
    "        \"Dugast's U\": math.log(total_tokens)**2 / ((math.log(total_tokens) - math.log(unique_words)) + 1e-8),\n",
    "        \"Honoré's H\": 100 * math.log(total_tokens) / (1 - once/(unique_words + 1e-8)),\n",
    "        \"Entropy\": -sum(p * math.log(p) for p in prob_word),\n",
    "        \"Sichel’s S\": twice,\n",
    "        'named_entity_ratio': named_entity_ratio,\n",
    "        'context_entity_ratio': context_entity_ratio,\n",
    "        'flesch_reading_ease': flesch_reading_ease,\n",
    "        'flesch_kincaid': flesch_kincaid,\n",
    "        'summary_ari':summary_ari,\n",
    "        'coleman_index': coleman_index,\n",
    "        \"Simpson’s D\": sum((count/total_tokens)**2 for count in word_counts.values()),\n",
    "        'intersection': intersection,\n",
    "        \"subjectivity\": subjectivity,\n",
    "        'difference': difference,\n",
    "        'union': union,\n",
    "        'overlap_score': overlap_score,\n",
    "        'recall': recall,\n",
    "        'precision': precision,\n",
    "        'f1_score': f1_score,\n",
    "        'jaccard_similarity': jaccard_similarity,\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "def compute_similarity(row):\n",
    "    source_vector = nlp(row['prompt_text']).vector\n",
    "    answer_vector = nlp(row['text']).vector\n",
    "    embedding_similarity = cosine_similarity([source_vector], [answer_vector])[0][0]\n",
    "    euclidean = euclidean_distances([source_vector], [answer_vector])[0][0]\n",
    "    pearson = np.corrcoef(source_vector.ravel(), answer_vector.ravel())[0, 1]\n",
    "    \n",
    "    return pd.Series([embedding_similarity, euclidean, pearson])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53f6f3de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T04:24:31.840585Z",
     "iopub.status.busy": "2023-09-09T04:24:31.840182Z",
     "iopub.status.idle": "2023-09-09T04:24:31.855857Z",
     "shell.execute_reply": "2023-09-09T04:24:31.854664Z"
    },
    "papermill": {
     "duration": 0.029655,
     "end_time": "2023-09-09T04:24:31.858327",
     "exception": false,
     "start_time": "2023-09-09T04:24:31.828672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import TruncatedSVD,PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "def preprocess_data(summary_df, prompt_df):\n",
    "    train_df = summary_df.merge(right=prompt_df, how='inner', on='prompt_id')\n",
    "\n",
    "    print('processing text embeddings......')\n",
    "    train_df = train_df.assign(\n",
    "      full_text='Title:\\n' + train_df['prompt_title'] +\n",
    "                '\\nQuestion:\\n' + train_df['prompt_question'] +\n",
    "                '\\nText:\\n' + train_df['prompt_text']\n",
    "      )\n",
    "    \n",
    "    # processing lemmas\n",
    "    print('extracting features.....')\n",
    "    data = pd.DataFrame(train_df.apply(extract_features, axis=1).tolist())\n",
    "    \n",
    "    print('processing features.....')\n",
    "    # Process other columns\n",
    "    train_df['summary_punctuation'] = train_df['text'].apply(count_punc)\n",
    "    train_df['lemmas'] = train_df['text'].apply(extract_lemmas)\n",
    "    train_df['full_text'] = train_df['full_text'].apply(extract_lemmas)\n",
    "\n",
    "    cols = ['summary_punctuation',\n",
    "            'content', 'wording']\n",
    "\n",
    "    print('applying vectorizer.....')\n",
    "    # Vectorization\n",
    "    global vectorizer, pca\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "    transformer = TfidfTransformer()\n",
    "    transformed_df = transformer.fit_transform(vectorizer.fit_transform(train_df['full_text']))\n",
    "    summary_transformed = transformer.fit_transform(vectorizer.transform(train_df['lemmas']))\n",
    "\n",
    "    print('scaling features.....')\n",
    "    # Scaling\n",
    "    scaler = MinMaxScaler(feature_range=(0, 10))\n",
    "    X_summary_scaled = scaler.fit_transform(transformed_df.toarray())\n",
    "    summary_scaled = scaler.transform(summary_transformed.toarray())\n",
    "\n",
    "    print('decomposing features.......')\n",
    "    # PCA Decomposition\n",
    "    components = round(transformed_df.shape[1] * (40/100))\n",
    "    pca = TruncatedSVD(n_components=components,algorithm='arpack', random_state=42)\n",
    "    pca_bag = pca.fit_transform(X_summary_scaled)\n",
    "    summary_bag = pca.transform(summary_scaled)\n",
    "    similarity_df = train_df.apply(compute_similarity, axis=1)\n",
    "    similarity = similarity_df.values\n",
    "    \n",
    "    stacked_bag = np.hstack((pca_bag,summary_bag,similarity,data.values,train_df[cols].values))\n",
    "\n",
    "    print('collecting outputs.........')\n",
    "    student_ids = train_df['student_id'].values\n",
    "\n",
    "    return stacked_bag, student_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "301ea201",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T04:24:31.879112Z",
     "iopub.status.busy": "2023-09-09T04:24:31.878728Z",
     "iopub.status.idle": "2023-09-09T04:24:32.028569Z",
     "shell.execute_reply": "2023-09-09T04:24:32.027551Z"
    },
    "papermill": {
     "duration": 0.163406,
     "end_time": "2023-09-09T04:24:32.031323",
     "exception": false,
     "start_time": "2023-09-09T04:24:31.867917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "english = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d25fab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T04:24:32.051915Z",
     "iopub.status.busy": "2023-09-09T04:24:32.051504Z",
     "iopub.status.idle": "2023-09-09T05:25:49.285231Z",
     "shell.execute_reply": "2023-09-09T05:25:49.284067Z"
    },
    "papermill": {
     "duration": 3677.256437,
     "end_time": "2023-09-09T05:25:49.297324",
     "exception": false,
     "start_time": "2023-09-09T04:24:32.040887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing text embeddings......\n",
      "extracting features.....\n",
      "processing features.....\n",
      "applying vectorizer.....\n",
      "scaling features.....\n",
      "decomposing features.......\n",
      "collecting outputs.........\n",
      "Preprocessed function executed in: 3677.23 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "stacked_bag, student_ids = preprocess_data(summary_df,prompt_df)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Preprocessed function executed in: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47fd7270",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T05:25:49.319198Z",
     "iopub.status.busy": "2023-09-09T05:25:49.318777Z",
     "iopub.status.idle": "2023-09-09T05:25:49.335519Z",
     "shell.execute_reply": "2023-09-09T05:25:49.334395Z"
    },
    "papermill": {
     "duration": 0.030899,
     "end_time": "2023-09-09T05:25:49.338191",
     "exception": false,
     "start_time": "2023-09-09T05:25:49.307292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(stacked_bag[:,:-2] ,stacked_bag[:,-2:],test_size=0.1,shuffle=True,random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9df56aa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T05:25:49.360430Z",
     "iopub.status.busy": "2023-09-09T05:25:49.359993Z",
     "iopub.status.idle": "2023-09-09T05:25:49.364945Z",
     "shell.execute_reply": "2023-09-09T05:25:49.364176Z"
    },
    "papermill": {
     "duration": 0.019434,
     "end_time": "2023-09-09T05:25:49.367714",
     "exception": false,
     "start_time": "2023-09-09T05:25:49.348280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bcb8305",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T05:25:49.390562Z",
     "iopub.status.busy": "2023-09-09T05:25:49.389756Z",
     "iopub.status.idle": "2023-09-09T05:25:49.395851Z",
     "shell.execute_reply": "2023-09-09T05:25:49.394873Z"
    },
    "papermill": {
     "duration": 0.020128,
     "end_time": "2023-09-09T05:25:49.398106",
     "exception": false,
     "start_time": "2023-09-09T05:25:49.377978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mcrmse(y_true, y_pred):\n",
    "    return np.mean(np.sqrt(np.mean((y_true - y_pred)**2, axis=0)))\n",
    "\n",
    "mcrmse_scorer = make_scorer(mcrmse, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aae3b67f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T05:25:49.421203Z",
     "iopub.status.busy": "2023-09-09T05:25:49.420251Z",
     "iopub.status.idle": "2023-09-09T05:25:49.428632Z",
     "shell.execute_reply": "2023-09-09T05:25:49.427509Z"
    },
    "papermill": {
     "duration": 0.022225,
     "end_time": "2023-09-09T05:25:49.430770",
     "exception": false,
     "start_time": "2023-09-09T05:25:49.408545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6448, 713)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8713c6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T05:25:49.454210Z",
     "iopub.status.busy": "2023-09-09T05:25:49.453490Z",
     "iopub.status.idle": "2023-09-09T05:49:33.099234Z",
     "shell.execute_reply": "2023-09-09T05:49:33.097937Z"
    },
    "papermill": {
     "duration": 1423.67276,
     "end_time": "2023-09-09T05:49:33.113869",
     "exception": false,
     "start_time": "2023-09-09T05:25:49.441109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training model: 0it [00:00, ?it/s]/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "training model: 20it [23:42, 71.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed function executed in: 1422.62 seconds\n",
      "Mean Score: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "start_time = time.time()\n",
    "\n",
    "best_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 600,\n",
    "    'max_depth': 11,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree': 0.6,\n",
    "#     'objective': 'reg:squarederror'\n",
    "}\n",
    "\n",
    "best_models = []\n",
    "\n",
    "score = []\n",
    "kfold = KFold(n_splits=20, shuffle=True, random_state=64)\n",
    "for train_idx, test_idx in tqdm(kfold.split(X_train, y_train), desc='training model'):\n",
    "    X, y = X_train[train_idx], y_train[train_idx]\n",
    "    test,test_y = X_train[test_idx], y_train[test_idx]\n",
    "    pipeline = MultiOutputRegressor(make_pipeline(\n",
    "#         MinMaxScaler(feature_range=(0,10)),\n",
    "        lgb.LGBMRegressor(**best_params, random_state=64)),\n",
    "        n_jobs=-1\n",
    "        )\n",
    "    pipeline.fit(X, y)\n",
    "    score.append(mcrmse(test_y,pipeline.predict(test)))\n",
    "    best_models.append(pipeline)\n",
    "    \n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time \n",
    "print(f\"Preprocessed function executed in: {elapsed_time:.2f} seconds\")\n",
    "print(f'Mean Score: {np.mean(score):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79277e84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T05:49:33.142596Z",
     "iopub.status.busy": "2023-09-09T05:49:33.142181Z",
     "iopub.status.idle": "2023-09-09T05:49:40.435082Z",
     "shell.execute_reply": "2023-09-09T05:49:40.434051Z"
    },
    "papermill": {
     "duration": 7.310991,
     "end_time": "2023-09-09T05:49:40.437952",
     "exception": false,
     "start_time": "2023-09-09T05:49:33.126961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Score: 0.47\n"
     ]
    }
   ],
   "source": [
    "best_scores = []\n",
    "for i in range(len(best_models)):\n",
    "    best_model = best_models[i]\n",
    "    y_preds = best_model.predict(X_test)\n",
    "    best_scores.append(mcrmse(y_test,y_preds))\n",
    "print(f'Mean Score: {np.mean(best_scores):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ee2b2db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T05:49:40.467216Z",
     "iopub.status.busy": "2023-09-09T05:49:40.466791Z",
     "iopub.status.idle": "2023-09-09T05:49:40.472036Z",
     "shell.execute_reply": "2023-09-09T05:49:40.470717Z"
    },
    "papermill": {
     "duration": 0.02287,
     "end_time": "2023-09-09T05:49:40.474315",
     "exception": false,
     "start_time": "2023-09-09T05:49:40.451445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_idx = np.argmin(best_scores)\n",
    "best_model = best_models[best_model_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8facbe2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T05:49:40.503412Z",
     "iopub.status.busy": "2023-09-09T05:49:40.502937Z",
     "iopub.status.idle": "2023-09-09T05:49:40.889757Z",
     "shell.execute_reply": "2023-09-09T05:49:40.888582Z"
    },
    "papermill": {
     "duration": 0.404866,
     "end_time": "2023-09-09T05:49:40.892674",
     "exception": false,
     "start_time": "2023-09-09T05:49:40.487808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47128652198943344"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = best_model.predict(X_test)\n",
    "mcrmse(y_test,y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dff8d16",
   "metadata": {
    "papermill": {
     "duration": 0.013437,
     "end_time": "2023-09-09T05:49:40.919995",
     "exception": false,
     "start_time": "2023-09-09T05:49:40.906558",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Preprocessing for text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c332cfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T05:49:40.949754Z",
     "iopub.status.busy": "2023-09-09T05:49:40.948827Z",
     "iopub.status.idle": "2023-09-09T05:49:40.962043Z",
     "shell.execute_reply": "2023-09-09T05:49:40.960639Z"
    },
    "papermill": {
     "duration": 0.031354,
     "end_time": "2023-09-09T05:49:40.964890",
     "exception": false,
     "start_time": "2023-09-09T05:49:40.933536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(summary_df, prompt_df):\n",
    "    train_df = summary_df.merge(right=prompt_df, how='inner', on='prompt_id')\n",
    "\n",
    "    print('processing text embeddings......')\n",
    "    train_df = train_df.assign(\n",
    "      full_text='Title:\\n' + train_df['prompt_title'] +\n",
    "                '\\nQuestion:\\n' + train_df['prompt_question'] +\n",
    "                '\\nText:\\n' + train_df['prompt_text']\n",
    "      )\n",
    "    \n",
    "    # processing lemmas\n",
    "    print('extracting features.....')\n",
    "    data = pd.DataFrame(train_df.apply(extract_features, axis=1).tolist())\n",
    "    \n",
    "    print('processing features.....')\n",
    "    # Process other columns\n",
    "    train_df['summary_punctuation'] = train_df['text'].apply(count_punc)\n",
    "    train_df['lemmas'] = train_df['text'].apply(extract_lemmas)\n",
    "    train_df['full_text'] = train_df['full_text'].apply(extract_lemmas)\n",
    "\n",
    "    cols = ['summary_punctuation']\n",
    "\n",
    "    print('applying vectorizer.....')\n",
    "    # Vectorization\n",
    "    transformer = TfidfTransformer()\n",
    "    transformed_df = transformer.fit_transform(vectorizer.transform(train_df['full_text']))\n",
    "    summary_transformed = transformer.fit_transform(vectorizer.transform(train_df['lemmas']))\n",
    "\n",
    "    print('scaling features.....')\n",
    "    # Scaling\n",
    "    scaler = MinMaxScaler(feature_range=(0, 10))\n",
    "    X_summary_scaled = scaler.fit_transform(transformed_df.toarray())\n",
    "    summary_scaled = scaler.fit_transform(summary_transformed.toarray())\n",
    "\n",
    "    print('decomposing features.......')\n",
    "    # PCA Decomposition\n",
    "    pca_bag = pca.transform(X_summary_scaled)\n",
    "    summary_bag = pca.transform(summary_scaled)\n",
    "    similarity_df = train_df.apply(compute_similarity, axis=1)\n",
    "    similarity = similarity_df.values\n",
    "    \n",
    "    stacked_bag = np.hstack((pca_bag,summary_bag,similarity,data.values,train_df[cols].values))\n",
    "\n",
    "    print('collecting outputs.........')\n",
    "    student_ids = train_df['student_id'].values\n",
    "\n",
    "    return stacked_bag, student_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "690493eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T05:49:40.994528Z",
     "iopub.status.busy": "2023-09-09T05:49:40.993495Z",
     "iopub.status.idle": "2023-09-09T05:49:41.243389Z",
     "shell.execute_reply": "2023-09-09T05:49:41.242189Z"
    },
    "papermill": {
     "duration": 0.267314,
     "end_time": "2023-09-09T05:49:41.245942",
     "exception": false,
     "start_time": "2023-09-09T05:49:40.978628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing text embeddings......\n",
      "extracting features.....\n",
      "processing features.....\n",
      "applying vectorizer.....\n",
      "scaling features.....\n",
      "decomposing features.......\n",
      "collecting outputs.........\n"
     ]
    }
   ],
   "source": [
    "test_df,ids = preprocess_data(test_summary, test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6876fdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T05:49:41.275927Z",
     "iopub.status.busy": "2023-09-09T05:49:41.275511Z",
     "iopub.status.idle": "2023-09-09T05:49:41.479221Z",
     "shell.execute_reply": "2023-09-09T05:49:41.478357Z"
    },
    "papermill": {
     "duration": 0.222112,
     "end_time": "2023-09-09T05:49:41.481988",
     "exception": false,
     "start_time": "2023-09-09T05:49:41.259876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_preds = best_model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "256292a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T05:49:41.511838Z",
     "iopub.status.busy": "2023-09-09T05:49:41.511437Z",
     "iopub.status.idle": "2023-09-09T05:49:41.521459Z",
     "shell.execute_reply": "2023-09-09T05:49:41.520350Z"
    },
    "papermill": {
     "duration": 0.028295,
     "end_time": "2023-09-09T05:49:41.524181",
     "exception": false,
     "start_time": "2023-09-09T05:49:41.495886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(y_preds,columns=['content','wording'],index=ids).reset_index()\n",
    "df_test = df_test.rename({'index':'student_id'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e5a39ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-09T05:49:41.554311Z",
     "iopub.status.busy": "2023-09-09T05:49:41.553902Z",
     "iopub.status.idle": "2023-09-09T05:49:41.567698Z",
     "shell.execute_reply": "2023-09-09T05:49:41.566802Z"
    },
    "papermill": {
     "duration": 0.031942,
     "end_time": "2023-09-09T05:49:41.570228",
     "exception": false,
     "start_time": "2023-09-09T05:49:41.538286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_valid_float(x):\n",
    "    return isinstance(x, float) and x == x  # This checks that x is not NaN since NaN != NaN in Python.\n",
    "\n",
    "cols_to_check = ['wording', 'content']\n",
    "df_test[cols_to_check] = df_test[cols_to_check].applymap(lambda x: x if is_valid_float(x) else 0.0)\n",
    "\n",
    "df_test.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "kernelspec": {
  "display_name": "Python 3",
  "language": "python",
  "name": "python3"
 },
 "language_info": {
  "codemirror_mode": {
   "name": "ipython",
   "version": 3
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.6.4"
 },
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5146.969526,
   "end_time": "2023-09-09T05:49:44.320407",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-09-09T04:23:57.350881",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
