{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e298454",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-09-01T08:04:39.273414Z",
     "iopub.status.busy": "2023-09-01T08:04:39.272432Z",
     "iopub.status.idle": "2023-09-01T08:04:39.344981Z",
     "shell.execute_reply": "2023-09-01T08:04:39.343925Z"
    },
    "papermill": {
     "duration": 0.08957,
     "end_time": "2023-09-01T08:04:39.348168",
     "exception": false,
     "start_time": "2023-09-01T08:04:39.258598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/commonlit-evaluate-student-summaries/sample_submission.csv\n",
      "/kaggle/input/commonlit-evaluate-student-summaries/prompts_train.csv\n",
      "/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv\n",
      "/kaggle/input/commonlit-evaluate-student-summaries/summaries_train.csv\n",
      "/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/meta.json\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/__init__.py\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/tokenizer\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/LICENSES_SOURCES\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/LICENSE\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/README.md\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/meta.json\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/accuracy.json\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/config.cfg\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/attribute_ruler/patterns\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/vocab/vectors\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/vocab/key2row\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/vocab/vectors.cfg\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/vocab/strings.json\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/vocab/lookups.bin\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/parser/model\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/parser/moves\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/parser/cfg\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/senter/model\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/senter/cfg\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/ner/model\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/ner/moves\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/ner/cfg\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/lemmatizer/lookups/lookups.bin\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/tagger/model\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/tagger/cfg\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/tok2vec/model\n",
      "/kaggle/input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1/tok2vec/cfg\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18aaf004",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T08:04:39.369386Z",
     "iopub.status.busy": "2023-09-01T08:04:39.368981Z",
     "iopub.status.idle": "2023-09-01T08:04:58.187395Z",
     "shell.execute_reply": "2023-09-01T08:04:58.186361Z"
    },
    "papermill": {
     "duration": 18.832563,
     "end_time": "2023-09-01T08:04:58.190551",
     "exception": false,
     "start_time": "2023-09-01T08:04:39.357988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_md' (3.4.1) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.6.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"../input/en-core-web-md/en_core_web_md/en_core_web_md-3.4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "803b42d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T08:04:58.212725Z",
     "iopub.status.busy": "2023-09-01T08:04:58.211977Z",
     "iopub.status.idle": "2023-09-01T08:04:58.340543Z",
     "shell.execute_reply": "2023-09-01T08:04:58.339403Z"
    },
    "papermill": {
     "duration": 0.143141,
     "end_time": "2023-09-01T08:04:58.343546",
     "exception": false,
     "start_time": "2023-09-01T08:04:58.200405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_df = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_train.csv')\n",
    "prompt_df = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff65969e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T08:04:58.365973Z",
     "iopub.status.busy": "2023-09-01T08:04:58.365555Z",
     "iopub.status.idle": "2023-09-01T08:07:03.259619Z",
     "shell.execute_reply": "2023-09-01T08:07:03.258628Z"
    },
    "papermill": {
     "duration": 124.908212,
     "end_time": "2023-09-01T08:07:03.262401",
     "exception": false,
     "start_time": "2023-09-01T08:04:58.354189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from string import punctuation as punc\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from string import punctuation as punc\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "vector_size=256\n",
    "window=10\n",
    "train_df = summary_df.merge(right=prompt_df, how='inner', on='prompt_id')\n",
    "sentences = train_df.apply(lambda x: x['prompt_title'].lower().split() + x['prompt_question'].lower().split() + x['prompt_text'].lower().split() + x['text'].lower().split(), axis=1)\n",
    "model = Word2Vec(sentences, vector_size=vector_size, \n",
    "                 window=window, min_count=1, workers=4,sg=1)\n",
    "\n",
    "def get_embeddings(text):\n",
    "    word_vec = model.wv.get_mean_vector(text.lower().split())\n",
    "    return  word_vec\n",
    "\n",
    "def count_punc(text):\n",
    "    return len([p for p in text if p in punc])\n",
    "\n",
    "# Overlap Score\n",
    "def overlap(sample):\n",
    "    answer = set(sample['text'].lower().split())\n",
    "    context = set(sample['prompt_text'].lower().split())\n",
    "    intersection = len(context.intersection(answer))\n",
    "    difference = len(context.difference(answer))\n",
    "    union = len(context.union(answer))\n",
    "    overlap_score = float(intersection/len(context))\n",
    "    jaccard_similarity = float(intersection/union)\n",
    "    return intersection, difference, union, overlap_score, jaccard_similarity\n",
    "\n",
    "def tree_depth_from_token(token):\n",
    "    \"\"\"Calculate the depth of a subtree rooted at a token.\"\"\"\n",
    "    if not list(token.children):\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 + max(tree_depth_from_token(child) for child in token.children)\n",
    "\n",
    "def average_dependency_tree_depth(doc):\n",
    "    \"\"\"Compute the average depth of the dependency tree.\"\"\"\n",
    "    root_tokens = [tok for tok in doc if tok.dep_ == 'ROOT']\n",
    "    if not root_tokens:\n",
    "        return 0\n",
    "    depths = [tree_depth_from_token(token) for token in root_tokens]\n",
    "    return sum(depths) / len(depths)\n",
    "\n",
    "def extract_features(row):\n",
    "    doc = nlp(row['text'].lower())\n",
    "    context = nlp(row['prompt_text'].lower())\n",
    "    \n",
    "    # 1. Length of the summary\n",
    "    length = len(doc)\n",
    "    \n",
    "    # 2. Number of unique words\n",
    "    unique_words = len(set([token for token in doc]))\n",
    "    context_unique_words = len(set([token for token in context]))\n",
    "    \n",
    "    #3. counter\n",
    "    tokens = [token for token in doc]\n",
    "    total_tokens = len(tokens)\n",
    "    word_counts = Counter(tokens)\n",
    "    once = sum(1 for word, count in word_counts.items() if count == 1)\n",
    "    twice = sum(1 for word, count in word_counts.items() if count == 2)\n",
    "    \n",
    "    # 4. Named entities\n",
    "    num_entities = len(doc.ents)\n",
    "    context_num_entities = len(context.ents)\n",
    "    \n",
    "    # 5. Average word length\n",
    "    avg_word_len = round(sum([len(token.text) for token in doc])/length, ndigits=4)\n",
    "    context_word_len = round(sum([len(token.text) for token in context])/length, ndigits=4)\n",
    "    \n",
    "    # 6. Text Polarity\n",
    "    sentiment_polarity = TextBlob(row['text']).sentiment.polarity\n",
    "    \n",
    "    # 7. part of speech tags\n",
    "    num_nouns = len([token for token in doc if token.pos_ == \"NOUN\"])\n",
    "    num_verbs = len([token for token in doc if token.pos_ == \"VERB\"])\n",
    "    num_adverbs = len([token for token in doc if token.pos_ == \"ADV\"])\n",
    "    \n",
    "    # 8. Numerical entities\n",
    "    num_numerical_entities = len([ent for ent in doc.ents if ent.label_ == \"CARDINAL\"])\n",
    "    \n",
    "    # 9. Number of sentence\n",
    "    sentence = len([sent for sent in doc.sents])\n",
    "    context_sentence = len([sent for sent in context.sents])\n",
    "    avg_sentence = float(sentence / total_tokens)\n",
    "    avg_context_sentence = float(context_sentence / context_unique_words)\n",
    "    avg_unique_sentence = float(sentence / unique_words)\n",
    "    \n",
    "    # 10. root word depth\n",
    "    average_tree_depth = average_dependency_tree_depth(doc)\n",
    "    \n",
    "    # 11. Probabilities of each word\n",
    "    prob_word = [count/total_tokens for word, count in word_counts.items()]\n",
    "    \n",
    "    # 12. Stopwords\n",
    "    stops = len([token for token in doc if token.is_stop])\n",
    "    \n",
    "    results = {\n",
    "        \"nsubj\": 0,\n",
    "        \"amod\": 0,\n",
    "        \"advmod\": 0,\n",
    "        \"neg\": 0,\n",
    "        \"xcomp\": 0,\n",
    "        \"acomp\": 0,\n",
    "        'ROOT': 0\n",
    "    }\n",
    "    \n",
    "    for token in doc:\n",
    "        # Check if the token's dependency is one of the desired dependencies\n",
    "        if token.dep_ in results:\n",
    "            results[token.dep_] += 1\n",
    "    \n",
    "    # Organizing features in a dictionary\n",
    "    features = {\n",
    "        'length': length,\n",
    "        'num_chars': len(row['text']),\n",
    "        \"stowords\": stops,\n",
    "        'sentence': sentence,\n",
    "        \"context_sentence\": context_sentence,\n",
    "        \"avg_sentence\": avg_sentence,\n",
    "        \"avg_context_sentence\": avg_context_sentence,\n",
    "        'avg_unique_sentence':avg_unique_sentence,\n",
    "        'unique_words': unique_words,\n",
    "        \"context_unique_words\": context_unique_words,\n",
    "        'num_entities': num_entities,\n",
    "        \"context_num_entities\": context_num_entities,\n",
    "        'avg_word_len': avg_word_len,\n",
    "        \"context_word_len\": context_word_len,\n",
    "        \"num_nouns\": num_nouns,\n",
    "        \"num_verbs\": num_verbs,\n",
    "        \"num_adverbs\": num_adverbs,\n",
    "        \"num_numerical_entities\": num_numerical_entities,\n",
    "        \"sentiment_polarity\": sentiment_polarity,\n",
    "        \"nsubj\": results['nsubj'],\n",
    "        \"amod\": results['amod'],\n",
    "        \"advmod\": results['advmod'],\n",
    "        \"neg\": results['neg'],\n",
    "        \"xcomp\": results['xcomp'],\n",
    "        \"acomp\": results['acomp'],\n",
    "        \"average_dependency_tree_depth\": average_tree_depth,\n",
    "        \"TTR\": unique_words / (total_tokens + 1e-8),\n",
    "        \"RTTR\": unique_words / (math.sqrt(total_tokens) + 1e-8),\n",
    "        \"CTTR\": unique_words / (math.sqrt(total_tokens / 2) + 1e-8),\n",
    "        # MATTR would require a window-based approach and is thus more involved\n",
    "        \"Guiraud's R\": unique_words / (math.sqrt(total_tokens) + 1e-8),\n",
    "        \"Herdan's C\": math.log(unique_words) / (math.log(total_tokens) + 1e-8),\n",
    "        \"Dugast's U\": math.log(total_tokens)**2 / ((math.log(total_tokens) - math.log(unique_words)) + 1e-8),\n",
    "        \"Honoré's H\": 100 * math.log(total_tokens) / (1 - once/(unique_words + 1e-8)),\n",
    "        \"Entropy\": -sum(p * math.log(p) for p in prob_word), \n",
    "        \"Sichel’s S\": twice,\n",
    "        \"Simpson’s D\": sum((count/total_tokens)**2 for count in word_counts.values())\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def compute_similarity(row):\n",
    "    source_vector = model.wv.get_mean_vector(row['prompt_text'])\n",
    "    answer_vector = model.wv.get_mean_vector(row['text'])\n",
    "    embedding_similarity = cosine_similarity([source_vector], [answer_vector])[0][0]\n",
    "    euclidean = euclidean_distances([source_vector], [answer_vector])[0][0]\n",
    "    pearson = np.corrcoef(source_vector.ravel(), answer_vector.ravel())[0, 1]\n",
    "    \n",
    "    return pd.Series([embedding_similarity, euclidean, pearson])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6766c575",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T08:07:03.283492Z",
     "iopub.status.busy": "2023-09-01T08:07:03.282893Z",
     "iopub.status.idle": "2023-09-01T08:07:03.293668Z",
     "shell.execute_reply": "2023-09-01T08:07:03.292884Z"
    },
    "papermill": {
     "duration": 0.023887,
     "end_time": "2023-09-01T08:07:03.296096",
     "exception": false,
     "start_time": "2023-09-01T08:07:03.272209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "def preprocess_data(summary_df, prompt_df):\n",
    "    train_df = summary_df.merge(right=prompt_df, how='inner', on='prompt_id')\n",
    "\n",
    "    print('processing text embeddings......')\n",
    "    word_vec = np.asarray(train_df['text'].apply(get_embeddings).tolist(), dtype=np.float32)\n",
    "    context_vec = np.asarray(train_df['prompt_text'].apply(get_embeddings).tolist(), dtype=np.float32)\n",
    "    \n",
    "    # processing lemmas\n",
    "    print('extracting features.....')\n",
    "    data = pd.DataFrame(train_df.apply(extract_features, axis=1).tolist())\n",
    "    \n",
    "    print('processing features.....')\n",
    "    # Process other columns\n",
    "    train_df['intersection'], train_df['difference'], train_df['union'], train_df['overlap_score'], train_df['jaccard_similarity'] = zip(*train_df.apply(overlap, axis=1).tolist())\n",
    "    train_df['summary_punctuation'] = train_df['text'].apply(count_punc)\n",
    "\n",
    "    cols = ['summary_punctuation','intersection','difference','union', \n",
    "            'overlap_score', 'jaccard_similarity',\n",
    "            'content', 'wording']\n",
    "\n",
    "    print('applying vectorizer.....')\n",
    "    similarity_df = train_df.apply(compute_similarity, axis=1)\n",
    "    similarity = similarity_df.values\n",
    "    \n",
    "    stacked_bag = np.hstack((context_vec,word_vec,similarity,data.values,train_df[cols].values))\n",
    "\n",
    "    print('collecting outputs.........')\n",
    "    student_ids = train_df['student_id'].values\n",
    "\n",
    "    return stacked_bag, student_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee2f3396",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T08:07:03.316819Z",
     "iopub.status.busy": "2023-09-01T08:07:03.316419Z",
     "iopub.status.idle": "2023-09-01T08:32:06.072113Z",
     "shell.execute_reply": "2023-09-01T08:32:06.071307Z"
    },
    "papermill": {
     "duration": 1502.779096,
     "end_time": "2023-09-01T08:32:06.084650",
     "exception": false,
     "start_time": "2023-09-01T08:07:03.305554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing text embeddings......\n",
      "extracting features.....\n",
      "processing features.....\n",
      "applying vectorizer.....\n",
      "collecting outputs.........\n",
      "Preprocessed function executed in: 1502.75 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "stacked_bag, student_ids = preprocess_data(summary_df,prompt_df)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Preprocessed function executed in: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e0d1f0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T08:32:06.106921Z",
     "iopub.status.busy": "2023-09-01T08:32:06.105782Z",
     "iopub.status.idle": "2023-09-01T08:32:06.127606Z",
     "shell.execute_reply": "2023-09-01T08:32:06.126638Z"
    },
    "papermill": {
     "duration": 0.035786,
     "end_time": "2023-09-01T08:32:06.130360",
     "exception": false,
     "start_time": "2023-09-01T08:32:06.094574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(stacked_bag[:,:-2] ,stacked_bag[:,-2:],test_size=0.1,shuffle=True,random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dabce5be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T08:32:06.156430Z",
     "iopub.status.busy": "2023-09-01T08:32:06.154747Z",
     "iopub.status.idle": "2023-09-01T08:32:06.162691Z",
     "shell.execute_reply": "2023-09-01T08:32:06.161248Z"
    },
    "papermill": {
     "duration": 0.024498,
     "end_time": "2023-09-01T08:32:06.165139",
     "exception": false,
     "start_time": "2023-09-01T08:32:06.140641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d50fca7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T08:32:06.194804Z",
     "iopub.status.busy": "2023-09-01T08:32:06.193411Z",
     "iopub.status.idle": "2023-09-01T08:32:06.200195Z",
     "shell.execute_reply": "2023-09-01T08:32:06.199110Z"
    },
    "papermill": {
     "duration": 0.025173,
     "end_time": "2023-09-01T08:32:06.202851",
     "exception": false,
     "start_time": "2023-09-01T08:32:06.177678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mcrmse(y_true, y_pred):\n",
    "    return np.mean(np.sqrt(np.mean((y_true - y_pred)**2, axis=0)))\n",
    "\n",
    "mcrmse_scorer = make_scorer(mcrmse, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8afa9ff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T08:32:06.230852Z",
     "iopub.status.busy": "2023-09-01T08:32:06.229994Z",
     "iopub.status.idle": "2023-09-01T09:07:40.772470Z",
     "shell.execute_reply": "2023-09-01T09:07:40.771409Z"
    },
    "papermill": {
     "duration": 2134.574309,
     "end_time": "2023-09-01T09:07:40.790262",
     "exception": false,
     "start_time": "2023-09-01T08:32:06.215953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training model: 0it [00:00, ?it/s]/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "training model: 10it [35:34, 213.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Score: 0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "scores = []\n",
    "best_params = {'max_depth': 11,'max_features': 0.6,'min_samples_split': 10,'n_estimators': 400}\n",
    "model_rf = MultiOutputRegressor(RandomForestRegressor(**best_params,n_jobs=-1, random_state=48),n_jobs=-1)\n",
    "score = []\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=64)\n",
    "for train_idx, test_idx in tqdm(kfold.split(X_train, y_train), desc='training model'):\n",
    "    X, y = X_train[train_idx], y_train[train_idx]\n",
    "    test,test_y = X_train[test_idx], y_train[test_idx]\n",
    "    model_rf.fit(X, y)\n",
    "    score.append(mcrmse(test_y,model_rf.predict(test)))\n",
    "scores.append(np.mean(score))\n",
    "print('Mean Score: %.2f'%(np.mean(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73e65dc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T09:07:40.815789Z",
     "iopub.status.busy": "2023-09-01T09:07:40.815300Z",
     "iopub.status.idle": "2023-09-01T09:07:41.355430Z",
     "shell.execute_reply": "2023-09-01T09:07:41.354446Z"
    },
    "papermill": {
     "duration": 0.556652,
     "end_time": "2023-09-01T09:07:41.358268",
     "exception": false,
     "start_time": "2023-09-01T09:07:40.801616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_preds = model_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2d3fb91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T09:07:41.383393Z",
     "iopub.status.busy": "2023-09-01T09:07:41.382361Z",
     "iopub.status.idle": "2023-09-01T09:07:41.391807Z",
     "shell.execute_reply": "2023-09-01T09:07:41.390738Z"
    },
    "papermill": {
     "duration": 0.024168,
     "end_time": "2023-09-01T09:07:41.394041",
     "exception": false,
     "start_time": "2023-09-01T09:07:41.369873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4975205479277523"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcrmse(y_test,y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dafc6443",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T09:07:41.419046Z",
     "iopub.status.busy": "2023-09-01T09:07:41.418277Z",
     "iopub.status.idle": "2023-09-01T09:07:41.424043Z",
     "shell.execute_reply": "2023-09-01T09:07:41.423077Z"
    },
    "papermill": {
     "duration": 0.02071,
     "end_time": "2023-09-01T09:07:41.426261",
     "exception": false,
     "start_time": "2023-09-01T09:07:41.405551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6448, 557)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28e2b48f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T09:07:41.451567Z",
     "iopub.status.busy": "2023-09-01T09:07:41.451218Z",
     "iopub.status.idle": "2023-09-01T09:07:41.462146Z",
     "shell.execute_reply": "2023-09-01T09:07:41.461168Z"
    },
    "papermill": {
     "duration": 0.026492,
     "end_time": "2023-09-01T09:07:41.464299",
     "exception": false,
     "start_time": "2023-09-01T09:07:41.437807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "def preprocess_data(summary_df, prompt_df):\n",
    "    train_df = summary_df.merge(right=prompt_df, how='inner', on='prompt_id')\n",
    "\n",
    "    print('processing text embeddings......')\n",
    "    word_vec = np.asarray(train_df['text'].apply(get_embeddings).tolist(), dtype=np.float32)\n",
    "    context_vec = np.asarray(train_df['prompt_text'].apply(get_embeddings).tolist(), dtype=np.float32)\n",
    "    \n",
    "    # processing lemmas\n",
    "    print('extracting features.....')\n",
    "    data = pd.DataFrame(train_df.apply(extract_features, axis=1).tolist())\n",
    "    \n",
    "    print('processing features.....')\n",
    "    # Process other columns\n",
    "    train_df['intersection'], train_df['difference'], train_df['union'], train_df['overlap_score'], train_df['jaccard_similarity'] = zip(*train_df.apply(overlap, axis=1).tolist())\n",
    "    train_df['summary_punctuation'] = train_df['text'].apply(count_punc)\n",
    "\n",
    "    cols = ['summary_punctuation','intersection','difference','union', \n",
    "            'overlap_score', 'jaccard_similarity']\n",
    "\n",
    "    print('applying vectorizer.....')\n",
    "    similarity_df = train_df.apply(compute_similarity, axis=1)\n",
    "    similarity = similarity_df.values\n",
    "    \n",
    "    stacked_bag = np.hstack((context_vec,word_vec,similarity,data.values,train_df[cols].values))\n",
    "\n",
    "    print('collecting outputs.........')\n",
    "    student_ids = train_df['student_id'].values\n",
    "\n",
    "    return stacked_bag, student_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6cc69c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T09:07:41.489069Z",
     "iopub.status.busy": "2023-09-01T09:07:41.488703Z",
     "iopub.status.idle": "2023-09-01T09:07:41.509336Z",
     "shell.execute_reply": "2023-09-01T09:07:41.508434Z"
    },
    "papermill": {
     "duration": 0.036076,
     "end_time": "2023-09-01T09:07:41.511890",
     "exception": false,
     "start_time": "2023-09-01T09:07:41.475814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_summary = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv')\n",
    "test_prompts = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d348adb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T09:07:41.538473Z",
     "iopub.status.busy": "2023-09-01T09:07:41.538111Z",
     "iopub.status.idle": "2023-09-01T09:07:41.625775Z",
     "shell.execute_reply": "2023-09-01T09:07:41.624404Z"
    },
    "papermill": {
     "duration": 0.104158,
     "end_time": "2023-09-01T09:07:41.628211",
     "exception": false,
     "start_time": "2023-09-01T09:07:41.524053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing text embeddings......\n",
      "extracting features.....\n",
      "processing features.....\n",
      "applying vectorizer.....\n",
      "collecting outputs.........\n"
     ]
    }
   ],
   "source": [
    "test_df,ids = preprocess_data(test_summary, test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91814870",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T09:07:41.654858Z",
     "iopub.status.busy": "2023-09-01T09:07:41.653864Z",
     "iopub.status.idle": "2023-09-01T09:07:42.008503Z",
     "shell.execute_reply": "2023-09-01T09:07:42.007572Z"
    },
    "papermill": {
     "duration": 0.370881,
     "end_time": "2023-09-01T09:07:42.011256",
     "exception": false,
     "start_time": "2023-09-01T09:07:41.640375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_preds = model_rf.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f97606a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T09:07:42.036453Z",
     "iopub.status.busy": "2023-09-01T09:07:42.036090Z",
     "iopub.status.idle": "2023-09-01T09:07:42.043903Z",
     "shell.execute_reply": "2023-09-01T09:07:42.042635Z"
    },
    "papermill": {
     "duration": 0.022634,
     "end_time": "2023-09-01T09:07:42.045951",
     "exception": false,
     "start_time": "2023-09-01T09:07:42.023317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(y_preds,columns=['content','wording'],index=ids).reset_index()\n",
    "df_test = df_test.rename({'index':'student_id'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70181d53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T09:07:42.070292Z",
     "iopub.status.busy": "2023-09-01T09:07:42.069948Z",
     "iopub.status.idle": "2023-09-01T09:07:42.081637Z",
     "shell.execute_reply": "2023-09-01T09:07:42.080887Z"
    },
    "papermill": {
     "duration": 0.026167,
     "end_time": "2023-09-01T09:07:42.083647",
     "exception": false,
     "start_time": "2023-09-01T09:07:42.057480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_valid_float(x):\n",
    "    return isinstance(x, float) and x == x  # This checks that x is not NaN since NaN != NaN in Python.\n",
    "\n",
    "cols_to_check = ['wording', 'content']\n",
    "df_test[cols_to_check] = df_test[cols_to_check].applymap(lambda x: x if is_valid_float(x) else 0.0)\n",
    "\n",
    "df_test.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "kernelspec": {
  "display_name": "Python 3",
  "language": "python",
  "name": "python3"
 },
 "language_info": {
  "codemirror_mode": {
   "name": "ipython",
   "version": 3
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.6.4"
 },
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3796.671915,
   "end_time": "2023-09-01T09:07:44.824392",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-09-01T08:04:28.152477",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
