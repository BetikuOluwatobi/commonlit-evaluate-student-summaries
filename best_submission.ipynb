{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.094074,"end_time":"2023-09-04T19:23:09.365133","exception":false,"start_time":"2023-09-04T19:23:09.271059","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-30T19:54:51.315112Z","iopub.execute_input":"2023-09-30T19:54:51.316495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install \"/kaggle/input/autocorrect/autocorrect-2.6.1.tar\"\n!pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\"","metadata":{"execution":{"iopub.status.idle":"2023-09-30T19:56:03.991511Z","shell.execute_reply.started":"2023-09-30T19:54:51.554644Z","shell.execute_reply":"2023-09-30T19:56:03.990090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport tensorflow_text as text\nfrom transformers import BertTokenizer\nimport torch\nimport spacy\nfrom tqdm import tqdm\nfrom spacy.tokens import Doc\nfrom nltk.tokenize import word_tokenize\nfrom spellchecker import SpellChecker\nimport re\nfrom autocorrect import Speller\n\n\nclass NLTKTokenizer(object):\n    def __init__(self, vocab):\n        self.vocab = vocab\n\n    def __call__(self, text):\n        # Tokenize the text using nltk's word_tokenize\n        words = word_tokenize(text)\n        \n        # Convert the list of words to a spaCy Doc object\n        return Doc(self.vocab, words=words)\n\n\nnlp = spacy.load(\"en_core_web_lg\")\n# Set spaCy's tokenizer to the custom tokenizer\nnlp.tokenizer = NLTKTokenizer(nlp.vocab)\nchecker = SpellChecker()\nspeller = Speller(lang='en')","metadata":{"papermill":{"duration":24.378919,"end_time":"2023-09-04T19:23:33.754836","exception":false,"start_time":"2023-09-04T19:23:09.375917","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-30T19:56:03.993586Z","iopub.execute_input":"2023-09-30T19:56:03.993969Z","iopub.status.idle":"2023-09-30T19:56:28.716810Z","shell.execute_reply.started":"2023-09-30T19:56:03.993936Z","shell.execute_reply":"2023-09-30T19:56:28.715729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary_df = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_train.csv')\nprompt_df = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_train.csv')\ntest_summary = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv')\ntest_prompts = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv')","metadata":{"papermill":{"duration":0.185061,"end_time":"2023-09-04T19:23:33.950970","exception":false,"start_time":"2023-09-04T19:23:33.765909","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-30T19:56:28.718413Z","iopub.execute_input":"2023-09-30T19:56:28.720308Z","iopub.status.idle":"2023-09-30T19:56:28.855735Z","shell.execute_reply.started":"2023-09-30T19:56:28.720240Z","shell.execute_reply":"2023-09-30T19:56:28.854928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_text as text\nfrom transformers import AutoTokenizer,TFAutoModel, pipeline\nfrom scipy.special import softmax\nimport torch\n\n\n\nSAVED_MODEL_DIR = '/kaggle/input/huggingfacedebertav3variants/deberta-v3-base'\ntokenizer = AutoTokenizer.from_pretrained(SAVED_MODEL_DIR)\nmodel = TFAutoModel.from_pretrained(SAVED_MODEL_DIR)#from_pt=True)\n\n# fe = pipeline('feature-extraction', tokenizer=tokenizer,model=model,torch_dtype=torch.float16,device=0)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:56:28.858180Z","iopub.execute_input":"2023-09-30T19:56:28.858534Z","iopub.status.idle":"2023-09-30T19:56:45.051389Z","shell.execute_reply.started":"2023-09-30T19:56:28.858504Z","shell.execute_reply":"2023-09-30T19:56:45.050215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = summary_df.merge(right=prompt_df,how='inner', on='prompt_id')","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:56:45.052794Z","iopub.execute_input":"2023-09-30T19:56:45.053168Z","iopub.status.idle":"2023-09-30T19:56:45.082035Z","shell.execute_reply.started":"2023-09-30T19:56:45.053137Z","shell.execute_reply":"2023-09-30T19:56:45.080895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs = tokenizer(train_df['text'][0],  max_length=128,padding='max_length',\n                    truncation=True,return_tensors='tf')\ntensor = model(**outputs)['last_hidden_state']\n# tf.reduce_mean(tensor, axis=1)[0].cpu().numpy().tolist()","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:56:45.084382Z","iopub.execute_input":"2023-09-30T19:56:45.085159Z","iopub.status.idle":"2023-09-30T19:56:46.210111Z","shell.execute_reply.started":"2023-09-30T19:56:45.085114Z","shell.execute_reply":"2023-09-30T19:56:46.208916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import time\n\n# BATCH_SIZE = 12\n\n# def compute_statistical_measures(embedding):\n#     output = np.mean(embedding), np.std(embedding), np.var(embedding), np.max(embedding)\n#     return output\n\n# def compute_tensor(text):\n#     outputs = tokenizer(text,  max_length=128,padding='max_length',\n#                     truncation=True,return_tensors='tf')\n#     tensor = model(**outputs)['last_hidden_state']\n#     return tf.reduce_mean(tensor, axis=1)[0].numpy().tolist()\n\n# def compute_scores(batch_texts, batch_prompts):\n#     features_batch = []\n#     for i in range(len(batch_texts)):\n#         output = compute_tensor(batch_texts[i])\n#         prompts_tensor = compute_tensor(batch_prompts[i])\n#         similarity = np.abs(tf.keras.losses.cosine_similarity(prompts_tensor,output).numpy())\n#         hinge = np.abs(tf.keras.losses.hinge(prompts_tensor,output).numpy())\n#         huber = np.abs(tf.keras.losses.huber(prompts_tensor,output).numpy())\n#         kld = np.abs(tf.keras.losses.kld(prompts_tensor,output).numpy())\n\n#         output.extend(compute_statistical_measures(output))\n#         output.extend(compute_statistical_measures(prompts_tensor))\n#         output.extend([similarity, hinge, huber, kld])\n\n#         features_batch.append(output)\n#     return features_batch\n\n# start = time.time()\n# scores = []\n# num_samples = len(train_df)\n# with tf.device('/GPU:0'):\n#     for start_idx in tqdm(range(0, num_samples, BATCH_SIZE), desc=\"processing feature extraction\"):\n#         end_idx = min(start_idx + BATCH_SIZE, num_samples)\n#         batch_texts = train_df.iloc[start_idx:end_idx]['text'].apply(speller).tolist()\n#         batch_prompts = train_df.iloc[start_idx:end_idx]['prompt_text'].tolist()\n\n#         scores_batch = compute_scores(batch_texts, batch_prompts)\n#         scores.extend(scores_batch)\n\n\n# bert_features = np.asarray(scores, dtype=np.float32)\n# end = time.time()\n# ellapsed_time = end - start\n# print(f'time ellapsed is: {ellapsed_time:.2f}ms')","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:56:46.211898Z","iopub.execute_input":"2023-09-30T19:56:46.212584Z","iopub.status.idle":"2023-09-30T19:56:46.219040Z","shell.execute_reply.started":"2023-09-30T19:56:46.212524Z","shell.execute_reply":"2023-09-30T19:56:46.218078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from string import punctuation as punc\nfrom collections import defaultdict\nimport spacy\nfrom string import punctuation as punc\nfrom tqdm import tqdm\nfrom gensim.models import Word2Vec\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\nimport spacy\nfrom textblob import TextBlob\nimport math\nfrom collections import Counter\nfrom nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize\nfrom tensorflow.keras import backend as K\n\n\n\ndef extract_lemmas(text):\n    doc = nlp(text)\n    lemmas = []\n\n    for token in doc:\n        if not token.is_stop:\n            lemmas.append(token.lemma_.lower())\n    return ' '.join(lemmas)\n\n\ndef count_punc(text):\n    return len([p for p in text if p in punc])\n\n# Overlap Score\n\ndef tree_depth_from_token(token):\n    \"\"\"Calculate the depth of a subtree rooted at a token.\"\"\"\n    if not list(token.children):\n        return 1\n    else:\n        return 1 + max(tree_depth_from_token(child) for child in token.children)\n\ndef average_dependency_tree_depth(doc):\n    \"\"\"Compute the average depth of the dependency tree.\"\"\"\n    root_tokens = [tok for tok in doc if tok.dep_ == 'ROOT']\n    if not root_tokens:\n        return 0\n    depths = [tree_depth_from_token(token) for token in root_tokens]\n    return sum(depths) / len(depths)\n\ndef process_text(doc):\n    # Using collections.Counter to count the occurrences efficiently\n    pos_counts = Counter([token.pos_ for token in doc if not token.is_stop])\n\n    # Extract counts directly using the dictionary\n    num_nouns = pos_counts.get(\"NOUN\", 0)\n    num_verbs = pos_counts.get(\"VERB\", 0)\n    num_adverbs = pos_counts.get(\"ADV\", 0)\n    stops = sum(1 for token in doc if token.is_stop)\n\n    return stops, num_nouns, num_verbs, num_adverbs\n\n\ndef syllable_count(word):\n    word = word.lower()\n    vowels = set(\"aeiouy\")\n    count = sum(1 for idx, char in enumerate(word) \n                if char in vowels and (idx == 0 or word[idx-1] not in vowels))\n\n    # Decrease count if word ends with an 'e' but is not preceded by a vowel\n    if word.endswith(\"e\") and (len(word) == 1 or word[-2] not in vowels):\n        count -= 1\n\n    # Ensure at least one syllable\n    count = max(1, count)\n    \n    return count\n\ndef extract_features(row, docs, context_docs):\n    idx = row.name\n    doc = docs[idx]\n    context = context_docs[idx]\n    \n    # Word Tokenize\n    tokens = [tok.text.lower() for tok in doc]\n    context_tokens = [tok.text.lower() for tok in context]\n    \n    # Capital Error\n    capital_error = sum(1 for sent in doc.sents if not sent.text[0].isupper())\n    \n    # 1. Length of the summary\n    length = len(tokens)\n    \n    # 2. Number of unique words\n    unique_words = len(set(tokens))\n    \n    #3. counter\n    total_tokens = len(tokens) if length > 0 else 1\n    word_counts = Counter(tokens)\n    once = sum(1 for word, count in word_counts.items() if count == 1)\n    twice = sum(1 for word, count in word_counts.items() if count == 2)\n    \n    # 4. Named entities\n    entities = set(ent.text.lower() for ent in doc.ents)\n    context_entities = set(ent.text.lower() for ent in context.ents)\n    num_entities = len(entities)\n    context_num_entities = len(context_entities)\n    \n    # Calculate the overlap between the two sets\n    overlap_entities = context_entities.intersection(entities)\n    \n    # Calculate the coverage score: (number of overlapping entities) / (number of entities in original text)\n    coverage_score = len(overlap_entities) / len(context_entities) if len(context_entities) > 0 else 0\n    num_overlap_entities = len(overlap_entities)\n    \n    named_entity_ratio = float(num_entities/total_tokens)\n    context_entity_ratio = float(context_num_entities/len(context_tokens))\n    \n    # 5. Average word length\n    avg_word_len = round(sum([len(token) for token in tokens])/total_tokens, ndigits=4)\n    context_avg_word_len = round(sum([len(token) for token in context_tokens])/len(context_tokens), ndigits=4)\n    \n    # 6. Summary Polarity\n    text_blob = TextBlob(row['text'])\n    context_blob = TextBlob(row['prompt_text'])\n    sentiment_polarity = text_blob.sentiment.polarity\n    subjectivity = text_blob.subjectivity\n    context_subjectivity = context_blob.subjectivity\n    context_polarity = context_blob.sentiment.polarity\n    error = sum(1 for token in tokens if token not in english)\n    error_ratio = error/total_tokens\n    \n    \n    # 7. Stopwords verbs and nouns\n    stops,num_nouns, num_verbs, num_adverbs = process_text(doc)\n    context_stops,context_nouns, context_verbs, context_adverbs = process_text(context)\n    \n    # 8. Numerical entities\n    num_numerical_entities = len([ent for ent in doc.ents if ent.label_ == \"CARDINAL\"])\n    \n    \n    # 9. Sentece count\n    sentences = [sent.text for sent in doc.sents]\n    sentence = len(sentences) if len(sentences) > 0 else 1\n    avg_sentence = float(sentence / total_tokens)\n    avg_unique_sentence = float(sentence /unique_words) if unique_words > 0 else 0.0\n    \n    avg_sentence_length = sum(len(sent) for sent in sentences) / sentence\n    max_sentence_length = max(len(sent) for sent in sentences)\n    min_sentence_length = min(len(sent) for sent in sentences)\n    std_sentence_length = (sum((len(sent) - avg_sentence_length)**2 for sent in sentences) / sentence)**0.5\n    \n    # 10 Tree Depth\n    average_tree_depth = average_dependency_tree_depth(doc)\n    prob_word = [count/total_tokens for word, count in word_counts.items()]\n    results = {\n        \"nsubj\": 0,\n        \"amod\": 0,\n        \"advmod\": 0,\n        \"xcomp\": 0,\n        \"acomp\": 0,\n        \"past\": 0,\n        \"present\": 0\n    }\n    \n    num_syllables = 0\n    \n    summary_punc = 0\n    for token in doc:\n        if \"VERB\" == token.pos_:\n            if \"VBD\" == token.tag_ or \"VBN\" == token.tag_:\n                results[\"past\"] += 1\n            elif token.tag_ in {\"VBG\", \"VBP\", \"VBZ\"}:\n                results[\"present\"] += 1\n        # Check if the token's dependency is one of the desired dependencies\n        if token.dep_ in results:\n            results[token.dep_] += 1\n            \n        if token.is_alpha:\n            num_syllables += syllable_count(token.text.lower())\n            \n        if token.is_punct:\n            summary_punc += 0\n    \n    # Unigram similarity\n    summary_set = set(tokens) if length > 0 else set()\n    context_set = set(context_tokens) if len(context_tokens) > 0 else set()\n    intersection = len(context_set.intersection(summary_set))\n    difference = len(context_set.difference(summary_set))\n    union = len(context_set.union(summary_set))\n    precision = float(intersection /unique_words) if unique_words > 0 else 0.0\n    recall = float(intersection / len(context_set)) if len(context_set) > 0 else 0.0\n    jaccard_similarity = float(intersection/union) if union > 0 else 0.0\n    overlap_score = intersection/float(min(len(context_set), len(summary_set)) + 1e-11)\n    f1_score = float(2 * ((precision * recall)/(precision + recall + 1e-11)))\n    \n    \n    # Bigram Similarity\n    answer_bigrams = set(list(ngrams(tokens, 2))) if length > 1 else set()\n    context_bigrams = set(list(ngrams(context_tokens, 2))) if len(context_tokens) > 1 else set()\n    intersection_bigram = len(answer_bigrams.intersection(context_bigrams))\n    union_bigram = len(answer_bigrams.union(context_bigrams))\n    precision_bigram = intersection_bigram /len(answer_bigrams) if len(answer_bigrams) > 0 else 0.0\n    recall_bigram = intersection_bigram / len(context_bigrams) if len(context_bigrams) > 0 else 0.0\n    jaccard_bigram = intersection_bigram/union_bigram if union_bigram > 0 else 0.0\n    bigram_overlap = intersection_bigram/float(min(len(context_bigrams), len(answer_bigrams)) + 1e-11)    \n    f1_bigram = 2 * ((precision_bigram * recall_bigram)/(precision_bigram + recall_bigram + 1e-11))\n    \n    # Trigram Similarity\n    answer_trigrams = set(list(ngrams(tokens, 3))) if length > 1 else set()\n    context_trigrams = set(list(ngrams(context_tokens, 3))) if len(context_tokens) > 1 else set()\n    intersection_trigram = len(answer_trigrams.intersection(context_trigrams))\n    union_trigram = len(answer_trigrams.union(context_trigrams))\n    precision_trigram = intersection_trigram / len(answer_trigrams) if len(answer_trigrams) > 0 else 0.0\n    recall_trigram = intersection_trigram / len(context_trigrams) if len(context_trigrams) > 0 else 0.0\n    jaccard_trigram = intersection_trigram/union_trigram if union_trigram > 0 else 0.0\n    trigram_overlap = intersection_trigram/float(min(len(context_trigrams), len(answer_trigrams)) + 1e-11)    \n    f1_trigram = 2 * ((precision_trigram * recall_trigram)/(precision_trigram + recall_trigram + 1e-11))\n    \n    # Readability\n    ASL = float(total_tokens / sentence)\n    ASW = float(num_syllables / total_tokens)\n\n    flesch_reading_ease = 206.835 - (1.015 * ASL) - (84.6 * ASW)\n    flesch_kincaid = (0.39 * ASL) + (11.8 * ASW) - 15.59\n    \n    # Automated Reading Index\n    characters = len(\"\".join(tokens))\n    summary_ari = 4.71 * (characters / total_tokens) + 0.5 * (total_tokens / sentence) - 21.43\n    \n    # Coleman-Liau Index Readability\n    # Where: L is the average number of letters per 100 words, S is the average number of sentences per 100 words\n    L = (characters / total_tokens) * 100\n    S = (sentence / total_tokens) * 100\n    coleman_index = (0.0588 * L) - (0.296 * S) - 15.8\n    \n    # Mispelt\n    mis_tokens = [token for token in checker.unknown(tokens) if token.isalpha()]\n    mispell_ratio = len(mis_tokens)/total_tokens\n    quotes = len(re.findall(r'\"(.*?)\"|\\'(.*?)\\'|“(.*?)”|‘(.*?)’|«(.*?)»|‹(.*?)›', row['text']))\n    \n    source_vector = context.vector\n    answer_vector = doc.vector\n    \n    embedding_similarity, euclidean, pearson = compute_similarity_score(source_vector, answer_vector)\n    \n    \n    # Organizing features in a dictionary\n    features = {\n        'length': length,\n        'num_chars': len(row['text']),\n        \"mispelt_tokens\": len(mis_tokens),\n        \"capital_error\": capital_error,\n        \"mispell_ratio\": mispell_ratio,\n        \"quotes\": quotes,\n        \"intersection_bigram\": intersection_bigram,\n        \"union_bigram\": union_bigram,\n        \"jaccard_bigram\": jaccard_bigram,\n        'recall_bigram': recall_bigram,\n        \"precision_bigram\": precision_bigram,\n        \"f1_bigram\": f1_bigram,\n        'bigram_overlap': bigram_overlap,\n        'sentence': sentence,\n        'avg_sentence_length': avg_sentence_length, \n        'max_sentence_length': max_sentence_length,\n        'min_sentence_length':min_sentence_length,\n        \"std_sentence_length\": std_sentence_length,\n        \"avg_sentence\": avg_sentence,\n        'avg_unique_sentence':avg_unique_sentence,\n        \"context_avg_word_len\": context_avg_word_len,\n        \"error\": error,\n        'error_ratio': error_ratio,\n        'unique_words': unique_words,\n        'num_entities': num_entities,\n        \"context_num_entities\": context_num_entities,\n        'coverage_score': coverage_score,\n        'num_overlap_entities': num_overlap_entities,\n        'avg_word_len': avg_word_len,\n        'intersection_trigram': intersection_trigram,\n        'union_trigram': union_trigram,\n        'recall_trigram': recall_trigram,\n        'precision_trigram': precision_trigram,\n        'jaccard_trigram': jaccard_trigram,\n        'trigram_overlap': trigram_overlap,\n        'f1_trigram': f1_trigram,\n        \"stops\": stops,\n        \"num_nouns\": num_nouns,\n        \"num_verbs\": num_verbs,\n        \"num_adverbs\": num_adverbs,\n        \"context_nouns\": context_nouns,\n        \"context_stops\": context_stops, \n        'context_verbs': context_verbs,\n        'context_adverbs': context_adverbs,\n        \"num_numerical_entities\": num_numerical_entities,\n        \"sentiment_polarity\": sentiment_polarity,\n        'context_polarity': context_polarity, \n        \"nsubj\": results['nsubj'],\n        \"amod\": results['amod'],\n        \"advmod\": results['advmod'],\n        \"xcomp\": results['xcomp'],\n        \"acomp\": results['acomp'],\n        \"past\": results[\"past\"],\n        \"present\": results[\"present\"],\n        \"average_dependency_tree_depth\": average_tree_depth,\n        \"TTR\": unique_words / (total_tokens + 1e-8),\n        \"RTTR\": unique_words / (math.sqrt(total_tokens) + 1e-8),\n        \"CTTR\": unique_words / (math.sqrt(total_tokens / 2) + 1e-8),\n        # MATTR would require a window-based approach and is thus more involved\n        \"Herdan's C\": math.log(unique_words) / (math.log(total_tokens) + 1e-8),\n        \"Dugast's U\": math.log(total_tokens)**2 / ((math.log(total_tokens) - math.log(unique_words)) + 1e-8),\n        \"Honoré's H\": 100 * math.log(total_tokens) / (1 - once/(unique_words + 1e-8)),\n        \"Entropy\": -sum(p * math.log(p) for p in prob_word),\n        \"Sichel’s S\": twice,\n        'named_entity_ratio': named_entity_ratio,\n        'context_entity_ratio': context_entity_ratio,\n        'flesch_reading_ease': flesch_reading_ease,\n        'flesch_kincaid': flesch_kincaid,\n        'summary_ari':summary_ari,\n        'coleman_index': coleman_index,\n        \"Simpson’s D\": sum((count/total_tokens)**2 for count in word_counts.values()),\n        'intersection': intersection,\n        \"subjectivity\": subjectivity,\n        \"context_subjectivity\": context_subjectivity,\n        'difference': difference,\n        'union': union,\n        'overlap_score': overlap_score,\n        'recall': recall,\n        'precision': precision,\n        'f1_score': f1_score,\n        'jaccard_similarity': jaccard_similarity,\n        'embedding_similarity': embedding_similarity,\n        'euclidean': euclidean, \n        'pearson': pearson,\n        'summary_punc': summary_punc,\n    }\n    \n    return features\n\ndef compute_similarity_score(source_vector, answer_vector):\n    embedding_similarity = cosine_similarity([source_vector], [answer_vector])[0][0]\n    euclidean = euclidean_distances([source_vector], [answer_vector])[0][0]\n    pearson = np.corrcoef(source_vector.ravel(), answer_vector.ravel())[0, 1]\n    \n    return embedding_similarity, euclidean, pearson\n\ndef compute_statistical_measures(embedding):\n    output = np.mean(embedding), np.std(embedding), np.var(embedding), np.max(embedding)\n    return output\n\ndef compute_scores(batch_texts,max_len=256):\n    features_batch = []\n    for i in range(len(batch_texts)):\n        with tf.device('/GPU:0'):\n            outputs = tokenizer(batch_texts[i], max_length=max_len,padding='max_length',\n                        truncation=True,return_tensors='tf')\n            tensor = model(**outputs)['last_hidden_state']\n        embed_max = tf.reduce_max(tensor, axis=1)[0].cpu().numpy().tolist()\n        embed_zero = tensor[0][0].cpu().numpy().tolist()\n        \n        embed_max.extend(compute_statistical_measures(embed_max))\n        embed_max.extend(compute_statistical_measures(embed_zero))\n\n        features_batch.append(embed_max)\n    return features_batch","metadata":{"papermill":{"duration":0.9004,"end_time":"2023-09-04T19:23:34.861798","exception":false,"start_time":"2023-09-04T19:23:33.961398","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-30T19:56:46.220800Z","iopub.execute_input":"2023-09-30T19:56:46.221511Z","iopub.status.idle":"2023-09-30T19:56:46.512463Z","shell.execute_reply.started":"2023-09-30T19:56:46.221471Z","shell.execute_reply":"2023-09-30T19:56:46.511161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\nimport re\n\ndef preprocess_data(summary_df, prompt_df, BATCH_SIZE=8, max_len=256):\n    train_df = summary_df.merge(right=prompt_df, how='inner', on='prompt_id')\n\n    print('processing text embeddings......')\n#     train_df = train_df.assign(\n#       full_text='Title:\\n' + train_df['prompt_title'] +\n#                 '\\nQuestion:\\n' + train_df['prompt_question'] +\n#                 '\\nContext:\\n' + train_df['text']\n#       )\n    \n    docs = list(nlp.pipe(train_df['text']))\n    context_docs = list(nlp.pipe(train_df['prompt_text']))\n    \n    # processing lemmas\n    print('extracting features.....')\n    drop_cols = [\n                \"mispelt_tokens\",\"mispell_ratio\",'error_ratio',\n                'context_nouns','context_entity_ratio',\n                'context_verbs', 'context_adverbs','context_subjectivity'\n    ]\n    data_cols = ['context_subjectivity', 'difference', 'recall',\n        'union','union_trigram', 'recall_trigram',\n        'union_bigram', 'recall_bigram',\n        'context_nouns','context_entity_ratio',\n        'context_verbs', 'context_adverbs']\n    \n    data = pd.DataFrame(train_df.apply(extract_features, args=(docs, context_docs), axis=1).tolist())\n    content_data = data.drop(drop_cols, axis=1)\n    data = data.drop(data_cols, axis=1)\n    \n    print('processing features.....')\n    # Process other columns\n    scores = []\n    num_samples = len(train_df)\n    for start_idx in tqdm(range(0, num_samples, BATCH_SIZE), desc=\"processing feature extraction\"):\n        end_idx = min(start_idx + BATCH_SIZE, num_samples)\n        batch_texts = train_df.iloc[start_idx:end_idx]['text'].apply(speller).tolist()\n        \n        scores_batch = compute_scores(batch_texts, max_len=max_len)\n        scores.extend(scores_batch)\n        K.clear_session()\n\n    bert_features = np.asarray(scores, dtype=np.float32)\n\n    content_bag = np.hstack((bert_features,content_data.values, train_df[['content']].values))\n    wording_bag = np.hstack((bert_features,data.values, train_df[['wording']].values))\n    \n    print('collecting outputs.........')\n    student_ids = train_df['student_id'].values\n\n    return content_bag, wording_bag, student_ids","metadata":{"papermill":{"duration":0.032381,"end_time":"2023-09-04T19:23:34.904770","exception":false,"start_time":"2023-09-04T19:23:34.872389","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-30T19:56:46.514058Z","iopub.execute_input":"2023-09-30T19:56:46.514426Z","iopub.status.idle":"2023-09-30T19:56:46.525418Z","shell.execute_reply.started":"2023-09-30T19:56:46.514394Z","shell.execute_reply":"2023-09-30T19:56:46.524364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import words\nenglish = set(words.words())","metadata":{"papermill":{"duration":0.176365,"end_time":"2023-09-04T19:23:35.091256","exception":false,"start_time":"2023-09-04T19:23:34.914891","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-30T19:56:46.527238Z","iopub.execute_input":"2023-09-30T19:56:46.527713Z","iopub.status.idle":"2023-09-30T19:56:46.675817Z","shell.execute_reply.started":"2023-09-30T19:56:46.527674Z","shell.execute_reply":"2023-09-30T19:56:46.674846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport time\n\nstart_time = time.time()\n\ncontent_bag, wording_bag, student_ids = preprocess_data(summary_df,prompt_df,BATCH_SIZE=8,max_len=128)\n\nend_time = time.time()\nelapsed_time = end_time - start_time\n\nprint(f\"Preprocessed function executed in: {elapsed_time:.2f} seconds\")","metadata":{"papermill":{"duration":3823.336513,"end_time":"2023-09-04T20:27:18.437851","exception":false,"start_time":"2023-09-04T19:23:35.101338","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-30T19:56:46.677466Z","iopub.execute_input":"2023-09-30T19:56:46.677865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3812.60 seconds","metadata":{}},{"cell_type":"code","source":"def split_data(df):\n    X_train,X_test,y_train,y_test = train_test_split(df[:,:-1] ,df[:,-1],test_size=0.1,shuffle=True,random_state=11)\n    return X_train,X_test,y_train,y_test","metadata":{"papermill":{"duration":0.034321,"end_time":"2023-09-04T20:27:18.483482","exception":false,"start_time":"2023-09-04T20:27:18.449161","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mcrmse(y_true, y_pred):\n    return np.mean(np.sqrt(np.mean((y_true - y_pred)**2, axis=0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport lightgbm as lgb\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\n\ndef train_model(df):\n    X_train,X_test,y_train,y_test = split_data(df)\n    start_time = time.time()\n\n    best_params = {\n        'learning_rate': 0.01,\n        'n_estimators': 600,\n        'max_depth': 11,\n        'subsample': 0.6,\n        'colsample_bytree': 0.6,\n        'objective': 'regression',\n        'metric': 'rmse'\n    }\n\n    best_models = []\n\n    score = []\n    kfold = KFold(n_splits=20, shuffle=True, random_state=64)\n    for train_idx, test_idx in tqdm(kfold.split(X_train, y_train), desc='training model'):\n        X, y = X_train[train_idx], y_train[train_idx]\n        test,test_y = X_train[test_idx], y_train[test_idx]\n        pipeline = make_pipeline(\n            MinMaxScaler(feature_range=(-10,10)),\n            lgb.LGBMRegressor(**best_params, random_state=64)\n        )\n        \n        pipeline.fit(X, y)\n        score.append(np.sqrt(mean_squared_error(test_y,pipeline.predict(test))))\n        best_models.append(pipeline)\n\n    print(f'Train Mean Score: {np.mean(score):.4f}')\n    \n    best_scores = []\n    for i in range(len(best_models)):\n        best_model = best_models[i]\n        y_preds = best_model.predict(X_test)\n        best_scores.append(np.sqrt(mean_squared_error(y_test,y_preds)))\n    print(f'Evaluation Mean Score: {np.mean(best_scores):.4f}')\n    \n    best_model_idx = np.argmin(best_scores)\n    best_model = best_models[best_model_idx]\n    \n    y_preds = best_model.predict(X_test)\n    score = np.sqrt(mean_squared_error(y_test,y_preds))\n    \n    end_time = time.time()\n    elapsed_time = end_time - start_time \n    print(f\"Preprocessed function executed in: {elapsed_time:.2f} seconds\")\n    print(f'Best Model Score: {score:.4f}')\n    \n    return best_model, y_test, y_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_content = np.hstack((content_bag[:,:768], content_bag[:,776:]))\nsample_wording = np.hstack((wording_bag[:,:768], wording_bag[:,776:]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"content_model, content_test, content_preds = train_model(sample_content)","metadata":{"papermill":{"duration":1275.871685,"end_time":"2023-09-04T20:48:34.470144","exception":false,"start_time":"2023-09-04T20:27:18.598459","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wording_model, wording_test, wording_preds  = train_model(sample_wording)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = np.hstack((content_test.reshape((-1,1)), wording_test.reshape((-1,1))))\npreds = np.hstack((content_preds.reshape((-1,1)), wording_preds.reshape((-1,1))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mcrmse(y_test, preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"0.3941, 0.5033\n0.4486827705476892","metadata":{}},{"cell_type":"code","source":"from joblib import dump\n\n# Save the pipeline to a file\ndump(content_model, 'content_model.joblib')\ndump(wording_model, 'wording_model.joblib')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from joblib import load\n\n# # Load the pipeline back from the file\n# content_model = load('content_model.joblib')\n# wording_model = load('wording_model.joblib')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing for text data","metadata":{"papermill":{"duration":0.013261,"end_time":"2023-09-04T20:48:34.956918","exception":false,"start_time":"2023-09-04T20:48:34.943657","status":"completed"},"tags":[]}},{"cell_type":"code","source":"K.clear_session()\ndef preprocess_data(summary_df, prompt_df, BATCH_SIZE=8, max_len=256):\n    train_df = summary_df.merge(right=prompt_df, how='inner', on='prompt_id')\n\n    print('processing text embeddings......')\n#     train_df = train_df.assign(\n#       full_text='Title:\\n' + train_df['prompt_title'] +\n#                 '\\nQuestion:\\n' + train_df['prompt_question'] +\n#                 '\\nContext:\\n' + train_df['text']\n#       )\n    \n    docs = list(nlp.pipe(train_df['text']))\n    context_docs = list(nlp.pipe(train_df['prompt_text']))\n    \n    # processing lemmas\n    print('extracting features.....')\n    drop_cols = [\n                \"mispelt_tokens\",\"mispell_ratio\",'error_ratio',\n                'context_nouns','context_entity_ratio',\n                'context_verbs', 'context_adverbs','context_subjectivity'\n    ]\n    data_cols = ['context_subjectivity', 'difference', 'recall',\n        'union','union_trigram', 'recall_trigram',\n        'union_bigram', 'recall_bigram',\n        'context_nouns','context_entity_ratio',\n        'context_verbs', 'context_adverbs']\n    \n    data = pd.DataFrame(train_df.apply(extract_features, args=(docs, context_docs), axis=1).tolist())\n    content_data = data.drop(drop_cols, axis=1)\n    data = data.drop(data_cols, axis=1)\n    \n    print('processing features.....')\n    # Process other columns\n    scores = []\n    num_samples = len(train_df)\n    for start_idx in tqdm(range(0, num_samples, BATCH_SIZE), desc=\"processing feature extraction\"):\n        end_idx = min(start_idx + BATCH_SIZE, num_samples)\n        batch_texts = train_df.iloc[start_idx:end_idx]['text'].apply(speller).tolist()\n        \n        scores_batch = compute_scores(batch_texts, max_len=max_len)\n        scores.extend(scores_batch)\n        K.clear_session()\n\n    bert_features = np.asarray(scores, dtype=np.float32)\n    \n    content_bag = np.hstack((bert_features,content_data.values))\n    wording_bag = np.hstack((bert_features,data.values))\n    \n    print('collecting outputs.........')\n    student_ids = train_df['student_id'].values\n\n    return content_bag, wording_bag, student_ids","metadata":{"papermill":{"duration":0.036424,"end_time":"2023-09-04T20:48:35.007448","exception":false,"start_time":"2023-09-04T20:48:34.971024","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"content, wording,ids = preprocess_data(test_summary, test_prompts,BATCH_SIZE=8,max_len=128)","metadata":{"papermill":{"duration":0.302293,"end_time":"2023-09-04T20:48:35.326067","exception":false,"start_time":"2023-09-04T20:48:35.023774","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cont_preds = content_model.predict(content).reshape((-1,1))\nword_preds = wording_model.predict(wording).reshape((-1,1))\ny_preds = np.hstack((cont_preds, word_preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.DataFrame(y_preds,columns=['content','wording'],index=ids).reset_index()\ndf_test = df_test.rename({'index':'student_id'},axis=1)","metadata":{"papermill":{"duration":0.027529,"end_time":"2023-09-04T20:48:35.610715","exception":false,"start_time":"2023-09-04T20:48:35.583186","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_valid_float(x):\n    return isinstance(x, float) and x == x  # This checks that x is not NaN since NaN != NaN in Python.\n\ncols_to_check = ['wording', 'content']\ndf_test[cols_to_check] = df_test[cols_to_check].applymap(lambda x: x if is_valid_float(x) else 0.0)\n\ndf_test.to_csv(\"submission.csv\", index=False)","metadata":{"papermill":{"duration":0.032411,"end_time":"2023-09-04T20:48:35.657275","exception":false,"start_time":"2023-09-04T20:48:35.624864","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}